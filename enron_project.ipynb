{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Email Project\n",
    "## June 2017, by Jude Moon\n",
    "<br />\n",
    "\n",
    "# Project Overview\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. \n",
    "\n",
    "In this project, I will play a detective, and put the new skills to use by building a person of interest (POI) identifier based on financial and email data made public as a result of the Enron scandal. I used [the provided dataset](https://github.com/udacity/ud120-projects/tree/master/final_project) from [Udacity Intro to Machine Learning Course](https://www.udacity.com/course/intro-to-machine-learning--ud120), which was combined with a hand-generated list of POI in the fraud case. POIs are individuals who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity.\n",
    "\n",
    "This document is to keep notes as I work through the project and compose answers to [a series of questions](https://docs.google.com/document/d/1NDgi1PrNJP7WTbfSUuRUnz8yzs5nGVTSzpO7oeNTEWA/pub?embedded=true) provided by Udacity, to show my thought processes and approaches to solve this problem.\n",
    "***\n",
    "\n",
    "# Part1. Data Exploration\n",
    "## Q1-1: Summarize the goal of this project\n",
    "The goal of the Enron project is to build a valid algorithm to identify Enron Employees who may have committed fraud (labeled as a person of interest, aka POI), using features from their financial and email datasets.\n",
    "\n",
    "## Q1-2: Give some background on the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pprint\n",
    "import operator\n",
    "import scipy.stats\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "#from feature_format import featureFormat, targetFeatureSplit\n",
    "import tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loads up the dataset (pickled dict of dicts)\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Enron dataset (emails + finances) has the form:\n",
    "    \n",
    "    data_dict[\"LASTNAME FIRSTNAME MIDDLEINITIAL\"] = { features_dict }\n",
    "    \n",
    "The data dictionary is stored as a **pickle** file, which is a handy way to store and load python objects directly.\n",
    "\n",
    "### How many data points (people) are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many POI?\n",
    "In other words, count the number of entries in the dictionary where\n",
    "data[person_name][\"poi\"]==1 \n",
    "- 1 means POI \n",
    "- 0 means non-POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POIs : 18\n",
      "Number of non-POIs : 128\n",
      "Percentage of POIs from the total : 12\n"
     ]
    }
   ],
   "source": [
    "count_poi = 0\n",
    "for person in data_dict:\n",
    "    if data_dict[person][\"poi\"] == 1:\n",
    "        count_poi += 1\n",
    "print \"Number of POIs : %i\" %count_poi\n",
    "print \"Number of non-POIs : %i\" %(146-count_poi)\n",
    "print \"Percentage of POIs from the total : %i\" %(count_poi*100/146) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we have sufficient data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st line: http://usatoday30.usatoday.com/money/industries/energy/2005-12-28-enron-participants_x.htm\n",
      "2nd line: \n",
      "3rd line: (y) Lay, Kenneth\n",
      "37th line: (n) Loehr, Christopher\n",
      "Number of POIs from Enron corpus: 35\n"
     ]
    }
   ],
   "source": [
    "# Udacity course provided a compiled list of all POI names from Enron corpus\n",
    "# poi_names.txt is newline delimited\n",
    "# read poi_names.txt file: each newline to string in a list\n",
    "poi_names_txt = open(\"poi_names.txt\", \"r\").read().splitlines()\n",
    "\n",
    "print \"1st line: \" + poi_names_txt[0]\n",
    "print \"2nd line: \" + poi_names_txt[1]\n",
    "print \"3rd line: \" + poi_names_txt[2]\n",
    "print \"37th line: \" + poi_names_txt[36]\n",
    "print \"Number of POIs from Enron corpus: %i\"%(len(poi_names_txt)-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name list of POIs which were extracted from Enron corpus database (emails of total 158 employees) showed 35 of POIs, whereas the combined dataset of financial and email data had 18 of POIs. \n",
    "\n",
    "About half of POIs were missing in the email + finance data dictionary. This might cause problems on understanding the full scope of patterns between features and POI. \n",
    "\n",
    "However, adding POIs data points from email data to financial data and leaving \"NaN\" value for all financial features of missing POIs would introduce \"NaN\" driving biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each person, how many features are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict[data_dict.keys()[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salary',\n",
      " 'to_messages',\n",
      " 'deferral_payments',\n",
      " 'total_payments',\n",
      " 'exercised_stock_options',\n",
      " 'bonus',\n",
      " 'restricted_stock',\n",
      " 'shared_receipt_with_poi',\n",
      " 'restricted_stock_deferred',\n",
      " 'total_stock_value',\n",
      " 'expenses',\n",
      " 'loan_advances',\n",
      " 'from_messages',\n",
      " 'other',\n",
      " 'from_this_person_to_poi',\n",
      " 'poi',\n",
      " 'director_fees',\n",
      " 'deferred_income',\n",
      " 'long_term_incentive',\n",
      " 'email_address',\n",
      " 'from_poi_to_this_person']\n"
     ]
    }
   ],
   "source": [
    "# the key of features for the first key\n",
    "features_list = data_dict[data_dict.keys()[0]].keys() \n",
    "pprint.pprint(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many NaN (Not a Number) exist per feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('poi', 0),\n",
      " ('total_stock_value', 20),\n",
      " ('total_payments', 21),\n",
      " ('email_address', 35),\n",
      " ('restricted_stock', 36),\n",
      " ('exercised_stock_options', 44),\n",
      " ('salary', 51),\n",
      " ('expenses', 51),\n",
      " ('other', 53),\n",
      " ('to_messages', 60),\n",
      " ('shared_receipt_with_poi', 60),\n",
      " ('from_messages', 60),\n",
      " ('from_poi_to_this_person', 60),\n",
      " ('from_this_person_to_poi', 60),\n",
      " ('bonus', 64),\n",
      " ('long_term_incentive', 80),\n",
      " ('deferred_income', 97),\n",
      " ('deferral_payments', 107),\n",
      " ('restricted_stock_deferred', 128),\n",
      " ('director_fees', 129),\n",
      " ('loan_advances', 142)]\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of feature and count of NaN pairs\n",
    "count_NaN = {}\n",
    "for feature in features_list:\n",
    "    count_NaN[feature] = 0\n",
    "\n",
    "for person in data_dict:\n",
    "    for feature in data_dict[person]:\n",
    "        if data_dict[person][feature] == \"NaN\":\n",
    "            count_NaN[feature] +=1\n",
    "\n",
    "# sort the dictionary by ascending ordering of values \n",
    "count_NaN = sorted(count_NaN.items(), key=operator.itemgetter(1))\n",
    "pprint.pprint(count_NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Would NaN introduce bias to the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\4jude\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:25: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaN_poi</th>\n",
       "      <th>NaN_total</th>\n",
       "      <th>NaN_non-poi</th>\n",
       "      <th>%NaN_in_poi</th>\n",
       "      <th>%NaN_in_non-poi</th>\n",
       "      <th>diff_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.40625</td>\n",
       "      <td>-41.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expenses</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.84375</td>\n",
       "      <td>-39.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>48.43750</td>\n",
       "      <td>-37.326389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>39.06250</td>\n",
       "      <td>-33.506944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income</th>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>90</td>\n",
       "      <td>38.888889</td>\n",
       "      <td>70.31250</td>\n",
       "      <td>-31.423611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email_address</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.34375</td>\n",
       "      <td>-27.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_term_incentive</th>\n",
       "      <td>6</td>\n",
       "      <td>80</td>\n",
       "      <td>74</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>57.81250</td>\n",
       "      <td>-24.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>35</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>27.34375</td>\n",
       "      <td>-21.788194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_messages</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>43.75000</td>\n",
       "      <td>-21.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>43.75000</td>\n",
       "      <td>-21.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_messages</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>43.75000</td>\n",
       "      <td>-21.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>43.75000</td>\n",
       "      <td>-21.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>43.75000</td>\n",
       "      <td>-21.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_payments</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.40625</td>\n",
       "      <td>-16.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_stock_value</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.62500</td>\n",
       "      <td>-15.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_advances</th>\n",
       "      <td>17</td>\n",
       "      <td>142</td>\n",
       "      <td>125</td>\n",
       "      <td>94.444444</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>-3.211806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferral_payments</th>\n",
       "      <td>13</td>\n",
       "      <td>107</td>\n",
       "      <td>94</td>\n",
       "      <td>72.222222</td>\n",
       "      <td>73.43750</td>\n",
       "      <td>-1.215278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>29.68750</td>\n",
       "      <td>3.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director_fees</th>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>111</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>13.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <td>18</td>\n",
       "      <td>128</td>\n",
       "      <td>110</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>85.93750</td>\n",
       "      <td>14.062500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           NaN_poi  NaN_total  NaN_non-poi  %NaN_in_poi  \\\n",
       "other                            0         53           53     0.000000   \n",
       "expenses                         0         51           51     0.000000   \n",
       "bonus                            2         64           62    11.111111   \n",
       "salary                           1         51           50     5.555556   \n",
       "deferred_income                  7         97           90    38.888889   \n",
       "email_address                    0         35           35     0.000000   \n",
       "long_term_incentive              6         80           74    33.333333   \n",
       "restricted_stock                 1         36           35     5.555556   \n",
       "to_messages                      4         60           56    22.222222   \n",
       "shared_receipt_with_poi          4         60           56    22.222222   \n",
       "from_messages                    4         60           56    22.222222   \n",
       "from_poi_to_this_person          4         60           56    22.222222   \n",
       "from_this_person_to_poi          4         60           56    22.222222   \n",
       "total_payments                   0         21           21     0.000000   \n",
       "total_stock_value                0         20           20     0.000000   \n",
       "loan_advances                   17        142          125    94.444444   \n",
       "deferral_payments               13        107           94    72.222222   \n",
       "poi                              0          0            0     0.000000   \n",
       "exercised_stock_options          6         44           38    33.333333   \n",
       "director_fees                   18        129          111   100.000000   \n",
       "restricted_stock_deferred       18        128          110   100.000000   \n",
       "\n",
       "                           %NaN_in_non-poi     diff_%  \n",
       "other                             41.40625 -41.406250  \n",
       "expenses                          39.84375 -39.843750  \n",
       "bonus                             48.43750 -37.326389  \n",
       "salary                            39.06250 -33.506944  \n",
       "deferred_income                   70.31250 -31.423611  \n",
       "email_address                     27.34375 -27.343750  \n",
       "long_term_incentive               57.81250 -24.479167  \n",
       "restricted_stock                  27.34375 -21.788194  \n",
       "to_messages                       43.75000 -21.527778  \n",
       "shared_receipt_with_poi           43.75000 -21.527778  \n",
       "from_messages                     43.75000 -21.527778  \n",
       "from_poi_to_this_person           43.75000 -21.527778  \n",
       "from_this_person_to_poi           43.75000 -21.527778  \n",
       "total_payments                    16.40625 -16.406250  \n",
       "total_stock_value                 15.62500 -15.625000  \n",
       "loan_advances                     97.65625  -3.211806  \n",
       "deferral_payments                 73.43750  -1.215278  \n",
       "poi                                0.00000   0.000000  \n",
       "exercised_stock_options           29.68750   3.645833  \n",
       "director_fees                     86.71875  13.281250  \n",
       "restricted_stock_deferred         85.93750  14.062500  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary showing the number of NaN and \n",
    "# number of POI with NaN each feature\n",
    "NaN_dict = {}\n",
    "keys = ['NaN_total', 'NaN_poi']\n",
    "\n",
    "for key in keys:\n",
    "    NaN_dict[key] = {}\n",
    "    for feature in features_list:\n",
    "        NaN_dict[key][feature] = 0\n",
    "        \n",
    "for person in data_dict:\n",
    "    for feature in data_dict[person]:\n",
    "        if data_dict[person][feature] == \"NaN\":\n",
    "            NaN_dict['NaN_total'][feature] +=1\n",
    "        \n",
    "        if data_dict[person][feature] == \"NaN\" and data_dict[person]['poi'] == True:\n",
    "            NaN_dict['NaN_poi'][feature] +=1\n",
    "\n",
    "# convert from a dictionary to a panda dataframe\n",
    "NaN_df = pd.DataFrame(NaN_dict)\n",
    "NaN_df['NaN_non-poi'] = NaN_df['NaN_total']-NaN_df['NaN_poi']\n",
    "NaN_df['%NaN_in_poi'] = (NaN_df['NaN_poi']/18)*100 # from total 18 POI\n",
    "NaN_df['%NaN_in_non-poi'] = (NaN_df['NaN_non-poi']/128)*100 # from total 128 non-POI\n",
    "NaN_df['diff_%'] = NaN_df['%NaN_in_poi'] - NaN_df['%NaN_in_non-poi']\n",
    "NaN_df = NaN_df.sort(['diff_%'])\n",
    "NaN_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought that features with a greater number of \"NaN\" value (e.g. 'loan_advances', 'director_fees', 'restricted_stock_deferred', etc.) would introduce bias. However, the disproportion in the numbers of \"NaN\" value between POI labeled group vs. non-POI labeled group might be more problematic. The features with large differences between % NaN in POI group vs. % NaN in non-POI group, for example, 'other' and 'expenses' are likely biased by \"NaN\" value. This means that if a supervised classification algorithm was to use 'other' as a feature, I would think that it might interpret \"NaN\" for 'other' as a clue that a person is a non-POI, so I would expect it to associate a \"NaN\" value with non-POI label.\n",
    "\n",
    "I am not sure whether it is ok to associate lack of information such as \"NaN\" value with a particular label. I will keep this in mind and consider excluding the NaN biased features at the feature selection stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of data exploration\n",
    "- Total number of data points: 146\n",
    "- Total number of data points labeled as POI: 18\n",
    "- Total number of data points labeled as non-POI: 126\n",
    "- Imbalanced classes\n",
    "- Number of missing POIs: 17\n",
    "- Number of initial features: 21\n",
    "- List of features with the number of \"NaN\" value greater than 73 (50% cut-off): \n",
    "\n",
    "| feature name  | number of NaN  |\n",
    "|:---:|:---:|\n",
    "| 'loan_advances' | 142  |\n",
    "| 'director_fees'  | 129  |\n",
    "| 'restricted_stock_deferred'  | 128  |\n",
    "|  'deferral_payments' | 107  |\n",
    "| 'deferred_income'  | 97  |\n",
    "| 'long_term_incentive'  |  80 |\n",
    "    \n",
    "\n",
    "- List of features with \"NaN\" value disproportionally distributed between POI vs. non-POI groups:\n",
    "\n",
    "|    feature_name   | NaN_total | NaN_poi | NaN_non-poi | %NaN_in_poi | %NaN_in_non-poi | %Difference|\n",
    "|:-----------------:|:---------:|:-------:|:-----------:|:-----------:|:---------------:|:---------------:|\n",
    "|      'other'      |     53    |    0    |      53     |      0      |        41       |       -41       |\n",
    "|     'expenses'    |     51    |    0    |      51     |      0      |        40       |       -40       |\n",
    "|      'bonus'      |     64    |    2    |      62     |      11     |        48       |       -37       |\n",
    "|      'salary'     |     51    |    1    |      50     |      6      |        39       |       -34       |\n",
    "| 'deferred_income' |     97    |    7    |      90     |      39     |        70       |       -31       |\n",
    "\n",
    "## Q1-3: How machine learning is useful in trying to accomplish the project goal and answer the project question\n",
    "\n",
    "It is uncertain that the existing financial and email dataset can provide good indicators/predictors in identifying POI. After data exploration, I realized that there are some limitations such as NaN driving bias and missing half of POIs. \n",
    "\n",
    "With these limitations and imperfect situation, machine learning can be useful in discovering some hidden patterns in features associated with POI labels and understanding relationship between a feature or a bundle of features and POI labels. After validating and evaluating the performance of machine learning algorithm, we can answer whether these simple numeric features can indicate or predict identification of POI. \n",
    "\n",
    "According to scikit-learn algorithm cheat-sheet below, predicting a category>yes>do you have labeled data>yes>less than 100k samples>yes> and the options are:\n",
    "\n",
    "\n",
    "- Linear SVC \n",
    "- KNeighbors \n",
    "- SVC ensemble    \n",
    "\n",
    "![image](http://scikit-learn.org/stable/_static/ml_map.png)\n",
    "\n",
    "To review on algorithms covered from Udacity lectures, I will also try:\n",
    "\n",
    "- Gaussian Naive Bayes\n",
    "- Decision Trees\n",
    "- Adaboost (boosted decision tree)\n",
    "- Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Investigation\n",
    "\n",
    "### Who has the most NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('WHALEY DAVID A', 18),\n",
      " ('WROBEL BRUCE', 18),\n",
      " ('THE TRAVEL AGENCY IN THE PARK', 18),\n",
      " ('GRAMM WENDY L', 18),\n",
      " ('LOCKHART EUGENE E', 20)]\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of person and count of NaN pairs\n",
    "missing_value = {}\n",
    "\n",
    "for person in data_dict:\n",
    "    missing_value[person] = 0\n",
    "    for feature in data_dict[person]:\n",
    "        if data_dict[person][feature] == \"NaN\":\n",
    "            missing_value[person] +=1\n",
    "\n",
    "# sort the dictionary by ascending ordering of values \n",
    "missing_value = sorted(missing_value.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# print top 5 those who have the most NaN\n",
    "pprint.pprint(missing_value[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glance at numerical variable distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to summary statistics of each feature, I use pandas dataframe\n",
    "# convert a python dictionary to a dataframe \n",
    "# with features as columns and people as rows\n",
    "df = pd.DataFrame(data_dict)\n",
    "df_trans = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.333474e+06</td>\n",
       "      <td>4.387965e+05</td>\n",
       "      <td>-3.827622e+05</td>\n",
       "      <td>1.942249e+04</td>\n",
       "      <td>4.182736e+06</td>\n",
       "      <td>7.074827e+04</td>\n",
       "      <td>358.602740</td>\n",
       "      <td>38.226027</td>\n",
       "      <td>24.287671</td>\n",
       "      <td>1.149658e+06</td>\n",
       "      <td>6.646839e+05</td>\n",
       "      <td>5.854318e+05</td>\n",
       "      <td>1.749257e+06</td>\n",
       "      <td>2.051637e+04</td>\n",
       "      <td>3.658114e+05</td>\n",
       "      <td>692.986301</td>\n",
       "      <td>1221.589041</td>\n",
       "      <td>4.350622e+06</td>\n",
       "      <td>5.846018e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.094029e+06</td>\n",
       "      <td>2.741325e+06</td>\n",
       "      <td>2.378250e+06</td>\n",
       "      <td>1.190543e+05</td>\n",
       "      <td>2.607040e+07</td>\n",
       "      <td>4.327163e+05</td>\n",
       "      <td>1441.259868</td>\n",
       "      <td>73.901124</td>\n",
       "      <td>79.278206</td>\n",
       "      <td>9.649342e+06</td>\n",
       "      <td>4.046072e+06</td>\n",
       "      <td>3.682345e+06</td>\n",
       "      <td>1.089995e+07</td>\n",
       "      <td>1.439661e+06</td>\n",
       "      <td>2.203575e+06</td>\n",
       "      <td>1072.969492</td>\n",
       "      <td>2226.770637</td>\n",
       "      <td>2.693448e+07</td>\n",
       "      <td>3.624681e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.025000e+05</td>\n",
       "      <td>-2.799289e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.604490e+06</td>\n",
       "      <td>-7.576788e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.409300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.792600e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.115000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.394475e+04</td>\n",
       "      <td>2.288695e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.082935e+05</td>\n",
       "      <td>2.018200e+04</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.595000e+02</td>\n",
       "      <td>3.605280e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.105960e+05</td>\n",
       "      <td>102.500000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>9.413595e+05</td>\n",
       "      <td>9.659550e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000e+05</td>\n",
       "      <td>9.684500e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.714221e+06</td>\n",
       "      <td>5.374075e+04</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>40.750000</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.750648e+05</td>\n",
       "      <td>1.506065e+05</td>\n",
       "      <td>8.145280e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.708505e+05</td>\n",
       "      <td>893.500000</td>\n",
       "      <td>1585.750000</td>\n",
       "      <td>1.968287e+06</td>\n",
       "      <td>2.319991e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.734362e+07</td>\n",
       "      <td>3.208340e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.398517e+06</td>\n",
       "      <td>3.117640e+08</td>\n",
       "      <td>5.235198e+06</td>\n",
       "      <td>14368.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>8.392500e+07</td>\n",
       "      <td>4.852193e+07</td>\n",
       "      <td>4.266759e+07</td>\n",
       "      <td>1.303223e+08</td>\n",
       "      <td>1.545629e+07</td>\n",
       "      <td>2.670423e+07</td>\n",
       "      <td>5521.000000</td>\n",
       "      <td>15149.000000</td>\n",
       "      <td>3.098866e+08</td>\n",
       "      <td>4.345095e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              bonus  deferral_payments  deferred_income  director_fees  \\\n",
       "count  1.460000e+02       1.460000e+02     1.460000e+02   1.460000e+02   \n",
       "mean   1.333474e+06       4.387965e+05    -3.827622e+05   1.942249e+04   \n",
       "std    8.094029e+06       2.741325e+06     2.378250e+06   1.190543e+05   \n",
       "min    0.000000e+00      -1.025000e+05    -2.799289e+07   0.000000e+00   \n",
       "25%    0.000000e+00       0.000000e+00    -3.792600e+04   0.000000e+00   \n",
       "50%    3.000000e+05       0.000000e+00     0.000000e+00   0.000000e+00   \n",
       "75%    8.000000e+05       9.684500e+03     0.000000e+00   0.000000e+00   \n",
       "max    9.734362e+07       3.208340e+07     0.000000e+00   1.398517e+06   \n",
       "\n",
       "       exercised_stock_options      expenses  from_messages  \\\n",
       "count             1.460000e+02  1.460000e+02     146.000000   \n",
       "mean              4.182736e+06  7.074827e+04     358.602740   \n",
       "std               2.607040e+07  4.327163e+05    1441.259868   \n",
       "min               0.000000e+00  0.000000e+00       0.000000   \n",
       "25%               0.000000e+00  0.000000e+00       0.000000   \n",
       "50%               6.082935e+05  2.018200e+04      16.500000   \n",
       "75%               1.714221e+06  5.374075e+04      51.250000   \n",
       "max               3.117640e+08  5.235198e+06   14368.000000   \n",
       "\n",
       "       from_poi_to_this_person  from_this_person_to_poi  loan_advances  \\\n",
       "count               146.000000               146.000000   1.460000e+02   \n",
       "mean                 38.226027                24.287671   1.149658e+06   \n",
       "std                  73.901124                79.278206   9.649342e+06   \n",
       "min                   0.000000                 0.000000   0.000000e+00   \n",
       "25%                   0.000000                 0.000000   0.000000e+00   \n",
       "50%                   2.500000                 0.000000   0.000000e+00   \n",
       "75%                  40.750000                13.750000   0.000000e+00   \n",
       "max                 528.000000               609.000000   8.392500e+07   \n",
       "\n",
       "       long_term_incentive         other  restricted_stock  \\\n",
       "count         1.460000e+02  1.460000e+02      1.460000e+02   \n",
       "mean          6.646839e+05  5.854318e+05      1.749257e+06   \n",
       "std           4.046072e+06  3.682345e+06      1.089995e+07   \n",
       "min           0.000000e+00  0.000000e+00     -2.604490e+06   \n",
       "25%           0.000000e+00  0.000000e+00      8.115000e+03   \n",
       "50%           0.000000e+00  9.595000e+02      3.605280e+05   \n",
       "75%           3.750648e+05  1.506065e+05      8.145280e+05   \n",
       "max           4.852193e+07  4.266759e+07      1.303223e+08   \n",
       "\n",
       "       restricted_stock_deferred        salary  shared_receipt_with_poi  \\\n",
       "count               1.460000e+02  1.460000e+02               146.000000   \n",
       "mean                2.051637e+04  3.658114e+05               692.986301   \n",
       "std                 1.439661e+06  2.203575e+06              1072.969492   \n",
       "min                -7.576788e+06  0.000000e+00                 0.000000   \n",
       "25%                 0.000000e+00  0.000000e+00                 0.000000   \n",
       "50%                 0.000000e+00  2.105960e+05               102.500000   \n",
       "75%                 0.000000e+00  2.708505e+05               893.500000   \n",
       "max                 1.545629e+07  2.670423e+07              5521.000000   \n",
       "\n",
       "        to_messages  total_payments  total_stock_value  \n",
       "count    146.000000    1.460000e+02       1.460000e+02  \n",
       "mean    1221.589041    4.350622e+06       5.846018e+06  \n",
       "std     2226.770637    2.693448e+07       3.624681e+07  \n",
       "min        0.000000    0.000000e+00      -4.409300e+04  \n",
       "25%        0.000000    9.394475e+04       2.288695e+05  \n",
       "50%      289.000000    9.413595e+05       9.659550e+05  \n",
       "75%     1585.750000    1.968287e+06       2.319991e+06  \n",
       "max    15149.000000    3.098866e+08       4.345095e+08  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get numerical statistics, replace string \"NaN\" to zero (0)\n",
    "def to_zero(v):\n",
    "    if v == 'NaN':\n",
    "        v = 0\n",
    "    return v\n",
    "df_trans = df_trans.applymap(to_zero)\n",
    "\n",
    "# check any numpy NaN\n",
    "print df_trans.isnull().sum().sum()\n",
    "\n",
    "# summary of variable distribution and center statistics\n",
    "df_trans.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-4: Are there any outliers in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bonus': ['LAVORATO JOHN J', 'TOTAL'],\n",
      " 'deferral_payments': ['FREVERT MARK A', 'TOTAL'],\n",
      " 'deferred_income': [],\n",
      " 'director_fees': ['BHATNAGAR SANJAY', 'TOTAL'],\n",
      " 'exercised_stock_options': ['LAY KENNETH L', 'TOTAL'],\n",
      " 'expenses': ['MCCLELLAN GEORGE', 'TOTAL'],\n",
      " 'from_messages': ['KAMINSKI WINCENTY J', 'KEAN STEVEN J'],\n",
      " 'from_poi_to_this_person': ['DIETRICH JANET R', 'LAVORATO JOHN J'],\n",
      " 'from_this_person_to_poi': ['DELAINEY DAVID W', 'LAVORATO JOHN J'],\n",
      " 'loan_advances': ['LAY KENNETH L', 'TOTAL'],\n",
      " 'long_term_incentive': ['MARTIN AMANDA K', 'TOTAL'],\n",
      " 'other': ['LAY KENNETH L', 'TOTAL'],\n",
      " 'restricted_stock': ['LAY KENNETH L', 'TOTAL'],\n",
      " 'restricted_stock_deferred': ['BELFER ROBERT', 'BHATNAGAR SANJAY'],\n",
      " 'salary': ['SKILLING JEFFREY K', 'TOTAL'],\n",
      " 'shared_receipt_with_poi': ['BELDEN TIMOTHY N', 'SHAPIRO RICHARD S'],\n",
      " 'to_messages': ['KEAN STEVEN J', 'SHAPIRO RICHARD S'],\n",
      " 'total_payments': ['LAY KENNETH L', 'TOTAL'],\n",
      " 'total_stock_value': ['LAY KENNETH L', 'TOTAL']}\n"
     ]
    }
   ],
   "source": [
    "# I defined outliers as being above of 99% quantile here\n",
    "# get lists of people above 99% quantile for each feature\n",
    "highest = {}\n",
    "for column in df_trans.columns:\n",
    "    if df_trans[column].dtypes == \"int64\":\n",
    "        highest[column]=[]\n",
    "        q = df_trans[column].quantile(0.99)\n",
    "        highest[column] = df_trans[df_trans[column] > q].index.tolist()\n",
    "    \n",
    "pprint.pprint(highest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the outliers repeatedly shown among the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DELAINEY DAVID W', 1),\n",
       " ('MARTIN AMANDA K', 1),\n",
       " ('SKILLING JEFFREY K', 1),\n",
       " ('BELDEN TIMOTHY N', 1),\n",
       " ('DIETRICH JANET R', 1),\n",
       " ('FREVERT MARK A', 1),\n",
       " ('KAMINSKI WINCENTY J', 1),\n",
       " ('BELFER ROBERT', 1),\n",
       " ('MCCLELLAN GEORGE', 1),\n",
       " ('KEAN STEVEN J', 2),\n",
       " ('BHATNAGAR SANJAY', 2),\n",
       " ('SHAPIRO RICHARD S', 2),\n",
       " ('LAVORATO JOHN J', 3),\n",
       " ('LAY KENNETH L', 6),\n",
       " ('TOTAL', 12)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize the previous dictionary, highest\n",
    "# create a dictionary of outliers and the frequency of being outlier\n",
    "highest_count = {}\n",
    "for feature in highest:\n",
    "    for person in highest[feature]:\n",
    "        if person not in highest_count:\n",
    "            highest_count[person] = 1\n",
    "        else:\n",
    "            highest_count[person] += 1\n",
    "            \n",
    "highest_count = sorted(highest_count.items(), key=operator.itemgetter(1))   \n",
    "highest_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Outlier Investigation\n",
    "\n",
    "- Top 5 people who has the most \"NaN\":\n",
    "\n",
    "|          person name          | number of NaN |\n",
    "|:-----------------------------:|:-------------:|\n",
    "|       LOCKHART EUGENE E       |       20      |\n",
    "|         GRAMM WENDY L         |       18      |\n",
    "| THE TRAVEL AGENCY IN THE PARK |       18      |\n",
    "|          WROBEL BRUCE         |       18      |\n",
    "|         WHALEY DAVID A        |       18      |\n",
    "\n",
    "- Top 3 people repeatedly shown as outliers:\n",
    "\n",
    "|   person name   | frequency of being outlier |\n",
    "|:---------------:|:--------------------------:|\n",
    "|      TOTAL      |             12             |\n",
    "|  LAY KENNETH L  |              6             |\n",
    "| LAVORATO JOHN J |              3             |\n",
    "\n",
    "### Take a look at outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOCKHART EUGENE E</th>\n",
       "      <th>GRAMM WENDY L</th>\n",
       "      <th>THE TRAVEL AGENCY IN THE PARK</th>\n",
       "      <th>WROBEL BRUCE</th>\n",
       "      <th>WHALEY DAVID A</th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>LAY KENNETH L</th>\n",
       "      <th>LAVORATO JOHN J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bonus</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97343619</td>\n",
       "      <td>7000000</td>\n",
       "      <td>8000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferral_payments</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32083396</td>\n",
       "      <td>202911</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-27992891</td>\n",
       "      <td>-300000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director_fees</th>\n",
       "      <td>NaN</td>\n",
       "      <td>119292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1398517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email_address</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>john.lavorato@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139130</td>\n",
       "      <td>98718</td>\n",
       "      <td>311764000</td>\n",
       "      <td>34348384</td>\n",
       "      <td>4158995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expenses</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5235198</td>\n",
       "      <td>99832</td>\n",
       "      <td>49537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_messages</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>2585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_advances</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83925000</td>\n",
       "      <td>81525000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_term_incentive</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48521928</td>\n",
       "      <td>3600000</td>\n",
       "      <td>2035380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>362096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42667589</td>\n",
       "      <td>10359729</td>\n",
       "      <td>1552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poi</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130322299</td>\n",
       "      <td>14761694</td>\n",
       "      <td>1008149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7576788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26704229</td>\n",
       "      <td>1072321</td>\n",
       "      <td>339288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2411</td>\n",
       "      <td>3962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_messages</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4273</td>\n",
       "      <td>7259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_payments</th>\n",
       "      <td>NaN</td>\n",
       "      <td>119292</td>\n",
       "      <td>362096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>309886585</td>\n",
       "      <td>103559793</td>\n",
       "      <td>10425757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_stock_value</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139130</td>\n",
       "      <td>98718</td>\n",
       "      <td>434509511</td>\n",
       "      <td>49110078</td>\n",
       "      <td>5167144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          LOCKHART EUGENE E GRAMM WENDY L  \\\n",
       "bonus                                   NaN           NaN   \n",
       "deferral_payments                       NaN           NaN   \n",
       "deferred_income                         NaN           NaN   \n",
       "director_fees                           NaN        119292   \n",
       "email_address                           NaN           NaN   \n",
       "exercised_stock_options                 NaN           NaN   \n",
       "expenses                                NaN           NaN   \n",
       "from_messages                           NaN           NaN   \n",
       "from_poi_to_this_person                 NaN           NaN   \n",
       "from_this_person_to_poi                 NaN           NaN   \n",
       "loan_advances                           NaN           NaN   \n",
       "long_term_incentive                     NaN           NaN   \n",
       "other                                   NaN           NaN   \n",
       "poi                                   False         False   \n",
       "restricted_stock                        NaN           NaN   \n",
       "restricted_stock_deferred               NaN           NaN   \n",
       "salary                                  NaN           NaN   \n",
       "shared_receipt_with_poi                 NaN           NaN   \n",
       "to_messages                             NaN           NaN   \n",
       "total_payments                          NaN        119292   \n",
       "total_stock_value                       NaN           NaN   \n",
       "\n",
       "                          THE TRAVEL AGENCY IN THE PARK WROBEL BRUCE  \\\n",
       "bonus                                               NaN          NaN   \n",
       "deferral_payments                                   NaN          NaN   \n",
       "deferred_income                                     NaN          NaN   \n",
       "director_fees                                       NaN          NaN   \n",
       "email_address                                       NaN          NaN   \n",
       "exercised_stock_options                             NaN       139130   \n",
       "expenses                                            NaN          NaN   \n",
       "from_messages                                       NaN          NaN   \n",
       "from_poi_to_this_person                             NaN          NaN   \n",
       "from_this_person_to_poi                             NaN          NaN   \n",
       "loan_advances                                       NaN          NaN   \n",
       "long_term_incentive                                 NaN          NaN   \n",
       "other                                            362096          NaN   \n",
       "poi                                               False        False   \n",
       "restricted_stock                                    NaN          NaN   \n",
       "restricted_stock_deferred                           NaN          NaN   \n",
       "salary                                              NaN          NaN   \n",
       "shared_receipt_with_poi                             NaN          NaN   \n",
       "to_messages                                         NaN          NaN   \n",
       "total_payments                                   362096          NaN   \n",
       "total_stock_value                                   NaN       139130   \n",
       "\n",
       "                          WHALEY DAVID A      TOTAL          LAY KENNETH L  \\\n",
       "bonus                                NaN   97343619                7000000   \n",
       "deferral_payments                    NaN   32083396                 202911   \n",
       "deferred_income                      NaN  -27992891                -300000   \n",
       "director_fees                        NaN    1398517                    NaN   \n",
       "email_address                        NaN        NaN  kenneth.lay@enron.com   \n",
       "exercised_stock_options            98718  311764000               34348384   \n",
       "expenses                             NaN    5235198                  99832   \n",
       "from_messages                        NaN        NaN                     36   \n",
       "from_poi_to_this_person              NaN        NaN                    123   \n",
       "from_this_person_to_poi              NaN        NaN                     16   \n",
       "loan_advances                        NaN   83925000               81525000   \n",
       "long_term_incentive                  NaN   48521928                3600000   \n",
       "other                                NaN   42667589               10359729   \n",
       "poi                                False      False                   True   \n",
       "restricted_stock                     NaN  130322299               14761694   \n",
       "restricted_stock_deferred            NaN   -7576788                    NaN   \n",
       "salary                               NaN   26704229                1072321   \n",
       "shared_receipt_with_poi              NaN        NaN                   2411   \n",
       "to_messages                          NaN        NaN                   4273   \n",
       "total_payments                       NaN  309886585              103559793   \n",
       "total_stock_value                  98718  434509511               49110078   \n",
       "\n",
       "                                   LAVORATO JOHN J  \n",
       "bonus                                      8000000  \n",
       "deferral_payments                              NaN  \n",
       "deferred_income                                NaN  \n",
       "director_fees                                  NaN  \n",
       "email_address              john.lavorato@enron.com  \n",
       "exercised_stock_options                    4158995  \n",
       "expenses                                     49537  \n",
       "from_messages                                 2585  \n",
       "from_poi_to_this_person                        528  \n",
       "from_this_person_to_poi                        411  \n",
       "loan_advances                                  NaN  \n",
       "long_term_incentive                        2035380  \n",
       "other                                         1552  \n",
       "poi                                          False  \n",
       "restricted_stock                           1008149  \n",
       "restricted_stock_deferred                      NaN  \n",
       "salary                                      339288  \n",
       "shared_receipt_with_poi                       3962  \n",
       "to_messages                                   7259  \n",
       "total_payments                            10425757  \n",
       "total_stock_value                          5167144  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['LOCKHART EUGENE E', 'GRAMM WENDY L', \\\n",
    "    'THE TRAVEL AGENCY IN THE PARK', \\\n",
    "    'WROBEL BRUCE', 'WHALEY DAVID A', \\\n",
    "    'TOTAL', 'LAY KENNETH L', 'LAVORATO JOHN J']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-5: How to handle outliers?\n",
    "\n",
    "'TOTAL' seemed an outlier introduced by spreadsheet quirk. It was the sum of all entries from the [pdf financial data](enron61702insiderpay.pdf). It needs to be removed from the dataset.\n",
    "\n",
    "In addition, 'LOCKHART EUGENE E' might need to be removed as well because he does not have any value other than NaN and is labeled as non-POI. \n",
    "\n",
    "Among the outliers and data points with too many missing values, only 'LAY KENNETH L' was labeled as POI and he was chairman of the Enron board of directors. So, I think these extreme values for this individual have a meaningful reason, not introduced by typos or technical errors.\n",
    "\n",
    "'LAVORATO JOHN J' is an interesting individual who was recieved the largest bonus and the most frequently communicated with POI via emails, but he is not labeled as POI. So, I expect that this person would be lied near the border line of classification or tend to be mis-classified.\n",
    "\n",
    "I tend to keep the other outliers detected, including 'THE TRAVEL AGENCY IN THE PARK'. According to the footnote from the [pdf financial data](enron61702insiderpay.pdf), the travel agency was co-owned by the sister of Enron's former Chairman and I don't have solid reasons to exclude this from the dataset.\n",
    "\n",
    "- List of data points to remove:\n",
    "    \n",
    "    - 'TOTAL'\n",
    "    - 'LOCKHART EUGENE E'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### there's an outlier--remove it! \n",
    "data_dict.pop(\"TOTAL\", 0)\n",
    "data_dict.pop(\"LOCKHART EUGENE E\", 0)\n",
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of key was 146 - 1('TOTAL') - 1(all zeros) = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update dataframe excluding outliers\n",
    "df = pd.DataFrame(data_dict)\n",
    "df_trans = df.transpose()\n",
    "df_trans = df_trans.applymap(to_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part2. Feature Engineering\n",
    "\n",
    "As part of the project, I should attempt to engineer my own feature that does not come ready-made in the dataset. Before creating new features, I need to explore features. \n",
    "\n",
    "## Taka a look at features\n",
    "\n",
    "### 1. Email features\n",
    "\n",
    "    to_messages, from_poi_to_this_person, from_messages, from_this_person_to_poi, shared_receipt_with_poi\n",
    "\n",
    "\n",
    "Among 6 of email features, I think email_address can be removed to make all numerical features plus I don't think email_address will give any meaningful information in classifying the labels. \n",
    "\n",
    "\n",
    "### 2. Financial features can be grouped into two categories: payments and stock value\n",
    "\n",
    "| categories  | features with positive values                                                                        | features with negative values | summed to         |\n",
    "|-------------|------------------------------------------------------------------------------------------------------|-------------------------------|-------------------|\n",
    "| payments    | salary, bonus, long_term_incentive, deferral_payments, loan_advances, other, expenses, director_fees | deferred_income               | total_payments    |\n",
    "| stock value | exercised_stock_options, restricted_stock                                                            | restricted_stock_deferred     | total_stock_value |\n",
    "\n",
    "'total_payments' and 'total_stock_value' are the summary features of each category. They can either well represent the latent features of the two category or cancel out meaningful patterns of individual features. So, here are some potential ways I can engineer the features.\n",
    "\n",
    "## Brainstorm How to Treat Features\n",
    "\n",
    "### 1. Treat all the numerical features individually\n",
    "    - Feature transformation using PCA (requires feature scaling prior to PCA) then feature selection\n",
    "    - Feature selection directly without any transformation\n",
    "### 2. Treat the numerical features as 3 latent features (payment, stock, and email)\n",
    "    - Feature transformation using PCA separately (each latent feature has a set of PCA feature) then feature selection\n",
    "    - Relativization prior to PCA transformation then feature selection\n",
    "    - Relativization then feature selection\n",
    "\n",
    "**Relativization can be achieved two ways:**\n",
    "    1. feature/summed to\n",
    "    2. feature/(summed to - feature with negative values) because feature with negative values canceled out the sum\n",
    "                \n",
    "**For email features, create features relative fraction of messages exchanged with POI among total messages:**\n",
    "     1. (\"from_this_person_to_poi\" + \"from_poi_to_this_person\")/(\"from_messages\" + \"to_messages\")\n",
    "     2. \"from_poi_to_this_person\"/\"to_messages\n",
    "     3. \"from_this_person_to_poi\"/\"from_messages\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove features\n",
    "email_address is not numeric variable so I will remove this feature from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove column email_address from df_trans\n",
    "df_trans = df_trans.drop('email_address', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new features\n",
    "\n",
    "## Q2-1: what features to create and the rationale behind it\n",
    "I will create 12 new features of the relative values of payment and stock by using relativization method 1. and 3 new features of the fraction of emails exchanged with POI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['salary',\n",
       " 'to_messages',\n",
       " 'deferral_payments',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'bonus',\n",
       " 'restricted_stock',\n",
       " 'shared_receipt_with_poi',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value',\n",
       " 'expenses',\n",
       " 'loan_advances',\n",
       " 'from_messages',\n",
       " 'other',\n",
       " 'from_this_person_to_poi',\n",
       " 'director_fees',\n",
       " 'deferred_income',\n",
       " 'long_term_incentive',\n",
       " 'from_poi_to_this_person']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to seperate the POI label from feature_list and remove email_address\n",
    "label = ['poi']\n",
    "features_list.remove('poi')\n",
    "features_list.remove('email_address')\n",
    "print len(features_list)\n",
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rel_salary',\n",
       " 'rel_bonus',\n",
       " 'rel_long_term_incentive',\n",
       " 'rel_deferral_payments',\n",
       " 'rel_loan_advances',\n",
       " 'rel_other',\n",
       " 'rel_expenses',\n",
       " 'rel_director_fees',\n",
       " 'rel_deferred_income']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new features of relative values of each payment feature to total_payments\n",
    "payment_features = ['salary', 'bonus', 'long_term_incentive', \\\n",
    "                    'deferral_payments', 'loan_advances', 'other', \\\n",
    "                    'expenses', 'director_fees', 'deferred_income']\n",
    "\n",
    "rel_payment = []\n",
    "for feature in payment_features:\n",
    "    new_feature_name = 'rel_' + feature\n",
    "    df_trans[new_feature_name] = (df_trans[feature]/df_trans['total_payments']).replace([np.inf, -np.inf, np.nan], 0)\n",
    "    rel_payment.append(new_feature_name)\n",
    "\n",
    "print len(rel_payment)\n",
    "rel_payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['salary',\n",
       " 'bonus',\n",
       " 'long_term_incentive',\n",
       " 'deferral_payments',\n",
       " 'loan_advances',\n",
       " 'other',\n",
       " 'expenses',\n",
       " 'director_fees',\n",
       " 'deferred_income',\n",
       " 'total_payments']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payment_features.append('total_payments')\n",
    "print len(payment_features)\n",
    "payment_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rel_exercised_stock_options',\n",
       " 'rel_restricted_stock',\n",
       " 'rel_restricted_stock_deferred']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new features of relative values of each stock feature to total_stock_value\n",
    "stock_features = ['exercised_stock_options', 'restricted_stock', \\\n",
    "                  'restricted_stock_deferred']\n",
    "\n",
    "rel_stock = []\n",
    "for feature in stock_features:\n",
    "    new_feature_name = 'rel_' + feature\n",
    "    df_trans[new_feature_name] = (df_trans[feature]/df_trans['total_stock_value']).replace([np.inf, -np.inf, np.nan], 0)\n",
    "    rel_stock.append(new_feature_name)\n",
    "\n",
    "rel_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exercised_stock_options',\n",
       " 'restricted_stock',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_features.append('total_stock_value')\n",
    "stock_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['salary',\n",
       " 'bonus',\n",
       " 'long_term_incentive',\n",
       " 'deferral_payments',\n",
       " 'loan_advances',\n",
       " 'other',\n",
       " 'expenses',\n",
       " 'director_fees',\n",
       " 'deferred_income',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'restricted_stock',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_features = payment_features+stock_features\n",
    "print len(financial_features)\n",
    "financial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rel_salary',\n",
       " 'rel_bonus',\n",
       " 'rel_long_term_incentive',\n",
       " 'rel_deferral_payments',\n",
       " 'rel_loan_advances',\n",
       " 'rel_other',\n",
       " 'rel_expenses',\n",
       " 'rel_director_fees',\n",
       " 'rel_deferred_income',\n",
       " 'rel_exercised_stock_options',\n",
       " 'rel_restricted_stock',\n",
       " 'rel_restricted_stock_deferred']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_financial_features = rel_payment+rel_stock\n",
    "print len(rel_financial_features)\n",
    "rel_financial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new features of fraction of emails exchanged with POI\n",
    "df_trans['fraction_poi']=((df_trans['from_this_person_to_poi']+\\\n",
    "                          df_trans['from_poi_to_this_person'])/\\\n",
    "(df_trans['from_messages']+df_trans['to_messages'])).fillna(0)\n",
    "\n",
    "df_trans['fraction_to_poi']=(df_trans['from_this_person_to_poi']/\\\n",
    "df_trans['from_messages']).fillna(0)\n",
    "\n",
    "df_trans['fraction_from_poi']=(df_trans['from_poi_to_this_person']/\\\n",
    "df_trans['to_messages']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numeric feataure list which excludes email adress\n",
    "email_features = ['to_messages', 'from_poi_to_this_person', 'from_messages',\n",
    "                     'from_this_person_to_poi', 'shared_receipt_with_poi', \n",
    "                      'fraction_poi', 'fraction_to_poi', 'fraction_from_poi']\n",
    "len(email_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "total_features = financial_features + email_features\n",
    "rel_total_features = rel_financial_features + email_features\n",
    "\n",
    "print len(total_features)\n",
    "print len(rel_total_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>...</th>\n",
       "      <th>rel_other</th>\n",
       "      <th>rel_expenses</th>\n",
       "      <th>rel_director_fees</th>\n",
       "      <th>rel_deferred_income</th>\n",
       "      <th>rel_exercised_stock_options</th>\n",
       "      <th>rel_restricted_stock</th>\n",
       "      <th>rel_restricted_stock_deferred</th>\n",
       "      <th>fraction_poi</th>\n",
       "      <th>fraction_to_poi</th>\n",
       "      <th>fraction_from_poi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.440000e+02</td>\n",
       "      <td>1.440000e+02</td>\n",
       "      <td>1.440000e+02</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>1.440000e+02</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>1.440000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.759974e+05</td>\n",
       "      <td>2.220896e+05</td>\n",
       "      <td>-1.936833e+05</td>\n",
       "      <td>9980.319444</td>\n",
       "      <td>2.075802e+06</td>\n",
       "      <td>35375.340278</td>\n",
       "      <td>363.583333</td>\n",
       "      <td>38.756944</td>\n",
       "      <td>24.625000</td>\n",
       "      <td>5.828125e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108559</td>\n",
       "      <td>0.095527</td>\n",
       "      <td>5.914364</td>\n",
       "      <td>-6.082185</td>\n",
       "      <td>0.498924</td>\n",
       "      <td>0.403771</td>\n",
       "      <td>-0.049046</td>\n",
       "      <td>0.028493</td>\n",
       "      <td>0.109922</td>\n",
       "      <td>0.022672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.233155e+06</td>\n",
       "      <td>7.541013e+05</td>\n",
       "      <td>6.060111e+05</td>\n",
       "      <td>31300.575144</td>\n",
       "      <td>4.795513e+06</td>\n",
       "      <td>45309.303038</td>\n",
       "      <td>1450.675239</td>\n",
       "      <td>74.276769</td>\n",
       "      <td>79.778266</td>\n",
       "      <td>6.794472e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221239</td>\n",
       "      <td>0.240176</td>\n",
       "      <td>58.879276</td>\n",
       "      <td>58.868342</td>\n",
       "      <td>0.396188</td>\n",
       "      <td>0.473146</td>\n",
       "      <td>0.255201</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.185935</td>\n",
       "      <td>0.036417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.025000e+05</td>\n",
       "      <td>-3.504386e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-701.013514</td>\n",
       "      <td>-0.074502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.493526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.708600e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.077054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.082935e+05</td>\n",
       "      <td>20182.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.015768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.627935</td>\n",
       "      <td>0.284209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000e+05</td>\n",
       "      <td>8.535500e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.683580e+06</td>\n",
       "      <td>53328.250000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>41.250000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075646</td>\n",
       "      <td>0.055635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850136</td>\n",
       "      <td>0.650782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043337</td>\n",
       "      <td>0.198827</td>\n",
       "      <td>0.029918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.000000e+06</td>\n",
       "      <td>6.426990e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>137864.000000</td>\n",
       "      <td>3.434838e+07</td>\n",
       "      <td>228763.000000</td>\n",
       "      <td>14368.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>8.152500e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>701.013514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.493526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224352</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.217341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              bonus  deferral_payments  deferred_income  director_fees  \\\n",
       "count  1.440000e+02       1.440000e+02     1.440000e+02     144.000000   \n",
       "mean   6.759974e+05       2.220896e+05    -1.936833e+05    9980.319444   \n",
       "std    1.233155e+06       7.541013e+05     6.060111e+05   31300.575144   \n",
       "min    0.000000e+00      -1.025000e+05    -3.504386e+06       0.000000   \n",
       "25%    0.000000e+00       0.000000e+00    -3.708600e+04       0.000000   \n",
       "50%    3.000000e+05       0.000000e+00     0.000000e+00       0.000000   \n",
       "75%    8.000000e+05       8.535500e+03     0.000000e+00       0.000000   \n",
       "max    8.000000e+06       6.426990e+06     0.000000e+00  137864.000000   \n",
       "\n",
       "       exercised_stock_options       expenses  from_messages  \\\n",
       "count             1.440000e+02     144.000000     144.000000   \n",
       "mean              2.075802e+06   35375.340278     363.583333   \n",
       "std               4.795513e+06   45309.303038    1450.675239   \n",
       "min               0.000000e+00       0.000000       0.000000   \n",
       "25%               0.000000e+00       0.000000       0.000000   \n",
       "50%               6.082935e+05   20182.000000      17.500000   \n",
       "75%               1.683580e+06   53328.250000      53.000000   \n",
       "max               3.434838e+07  228763.000000   14368.000000   \n",
       "\n",
       "       from_poi_to_this_person  from_this_person_to_poi  loan_advances  \\\n",
       "count               144.000000               144.000000   1.440000e+02   \n",
       "mean                 38.756944                24.625000   5.828125e+05   \n",
       "std                  74.276769                79.778266   6.794472e+06   \n",
       "min                   0.000000                 0.000000   0.000000e+00   \n",
       "25%                   0.000000                 0.000000   0.000000e+00   \n",
       "50%                   4.000000                 0.000000   0.000000e+00   \n",
       "75%                  41.250000                14.000000   0.000000e+00   \n",
       "max                 528.000000               609.000000   8.152500e+07   \n",
       "\n",
       "             ...           rel_other  rel_expenses  rel_director_fees  \\\n",
       "count        ...          144.000000    144.000000         144.000000   \n",
       "mean         ...            0.108559      0.095527           5.914364   \n",
       "std          ...            0.221239      0.240176          58.879276   \n",
       "min          ...            0.000000      0.000000           0.000000   \n",
       "25%          ...            0.000000      0.000000           0.000000   \n",
       "50%          ...            0.000720      0.015768           0.000000   \n",
       "75%          ...            0.075646      0.055635           0.000000   \n",
       "max          ...            1.000000      1.000000         701.013514   \n",
       "\n",
       "       rel_deferred_income  rel_exercised_stock_options  rel_restricted_stock  \\\n",
       "count           144.000000                   144.000000            144.000000   \n",
       "mean             -6.082185                     0.498924              0.403771   \n",
       "std              58.868342                     0.396188              0.473146   \n",
       "min            -701.013514                    -0.074502              0.000000   \n",
       "25%              -0.077054                     0.000000              0.000000   \n",
       "50%               0.000000                     0.627935              0.284209   \n",
       "75%               0.000000                     0.850136              0.650782   \n",
       "max               0.000000                     1.000000              3.493526   \n",
       "\n",
       "       rel_restricted_stock_deferred  fraction_poi  fraction_to_poi  \\\n",
       "count                     144.000000    144.000000       144.000000   \n",
       "mean                       -0.049046      0.028493         0.109922   \n",
       "std                         0.255201      0.042827         0.185935   \n",
       "min                        -2.493526      0.000000         0.000000   \n",
       "25%                         0.000000      0.000000         0.000000   \n",
       "50%                         0.000000      0.008772         0.000000   \n",
       "75%                         0.000000      0.043337         0.198827   \n",
       "max                         0.000000      0.224352         1.000000   \n",
       "\n",
       "       fraction_from_poi  \n",
       "count         144.000000  \n",
       "mean            0.022672  \n",
       "std             0.036417  \n",
       "min             0.000000  \n",
       "25%             0.000000  \n",
       "50%             0.004952  \n",
       "75%             0.029918  \n",
       "max             0.217341  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trans.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0L"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check any numpy NaN\n",
    "df_trans.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Feature Exploration\n",
    "\n",
    "| List Name | Features | # of Features |\n",
    "|------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| features_list | ['salary','to_messages','deferral_payments','total_payments','exercised_stock_options','bonus', 'restricted_stock','shared_receipt_with_poi','restricted_stock_deferred','total_stock_value', 'expenses','loan_advances','from_messages','other','from_this_person_to_poi','director_fees', 'deferred_income','long_term_incentive','from_poi_to_this_person'] | 19 |\n",
    "| rel_payment | ['rel_salary','rel_bonus','rel_long_term_incentive','rel_deferral_payments','rel_loan_advances', 'rel_other','rel_expenses','rel_director_fees','rel_deferred_income'] | 9 |\n",
    "| payment_features | ['salary','bonus','long_term_incentive','deferral_payments','loan_advances','other','expenses', 'director_fees','deferred_income','total_payments'] | 10 |\n",
    "| rel_stock | ['rel_exercised_stock_options','rel_restricted_stock','rel_restricted_stock_deferred'] | 3 |\n",
    "| stock_features | ['exercised_stock_options','restricted_stock','restricted_stock_deferred','total_stock_value'] | 4 |\n",
    "| financial_features | payment_features+stock_features | 14 |\n",
    "| rel_financial_features | rel_payment+rel_stock | 12 |\n",
    "| email_features | ['to_messages', 'from_poi_to_this_person', 'from_messages','from_this_person_to_poi',  'shared_receipt_with_poi','fraction_poi', 'fraction_to_poi', 'fraction_from_poi'] | 8 |\n",
    "| total_features | financial_features + email_features | 22 |\n",
    "| rel_total_features | rel_financial_features + email_features | 20 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "## Q2-2: do I have to do any scaling? why or why not?\n",
    "Yes. I will use **MinMaxScaler** to adjust financial (in $) and email (count) features to be equally weighted and ranged between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_trans), \\\n",
    "                         index=df_trans.index, columns=df_trans.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 35)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.shape # returns length of array and length of item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>...</th>\n",
       "      <th>rel_other</th>\n",
       "      <th>rel_expenses</th>\n",
       "      <th>rel_director_fees</th>\n",
       "      <th>rel_deferred_income</th>\n",
       "      <th>rel_exercised_stock_options</th>\n",
       "      <th>rel_restricted_stock</th>\n",
       "      <th>rel_restricted_stock_deferred</th>\n",
       "      <th>fraction_poi</th>\n",
       "      <th>fraction_to_poi</th>\n",
       "      <th>fraction_from_poi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.049711</td>\n",
       "      <td>0.944731</td>\n",
       "      <td>0.072392</td>\n",
       "      <td>0.060434</td>\n",
       "      <td>0.154638</td>\n",
       "      <td>0.025305</td>\n",
       "      <td>0.073403</td>\n",
       "      <td>0.040435</td>\n",
       "      <td>0.007149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108559</td>\n",
       "      <td>0.095527</td>\n",
       "      <td>0.008437</td>\n",
       "      <td>0.991324</td>\n",
       "      <td>0.533666</td>\n",
       "      <td>0.115577</td>\n",
       "      <td>0.980331</td>\n",
       "      <td>0.127001</td>\n",
       "      <td>0.109922</td>\n",
       "      <td>0.104317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.154144</td>\n",
       "      <td>0.115492</td>\n",
       "      <td>0.172929</td>\n",
       "      <td>0.227040</td>\n",
       "      <td>0.139614</td>\n",
       "      <td>0.198062</td>\n",
       "      <td>0.100966</td>\n",
       "      <td>0.140676</td>\n",
       "      <td>0.130999</td>\n",
       "      <td>0.083342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221239</td>\n",
       "      <td>0.240176</td>\n",
       "      <td>0.083992</td>\n",
       "      <td>0.083976</td>\n",
       "      <td>0.368718</td>\n",
       "      <td>0.135435</td>\n",
       "      <td>0.102345</td>\n",
       "      <td>0.190891</td>\n",
       "      <td>0.185935</td>\n",
       "      <td>0.167558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015698</td>\n",
       "      <td>0.989417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999890</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.015698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017710</td>\n",
       "      <td>0.088222</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.015768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653732</td>\n",
       "      <td>0.081353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.017005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049015</td>\n",
       "      <td>0.233116</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075646</td>\n",
       "      <td>0.055635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860527</td>\n",
       "      <td>0.186282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.193167</td>\n",
       "      <td>0.198827</td>\n",
       "      <td>0.137655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            bonus  deferral_payments  deferred_income  director_fees  \\\n",
       "count  144.000000         144.000000       144.000000     144.000000   \n",
       "mean     0.084500           0.049711         0.944731       0.072392   \n",
       "std      0.154144           0.115492         0.172929       0.227040   \n",
       "min      0.000000           0.000000         0.000000       0.000000   \n",
       "25%      0.000000           0.015698         0.989417       0.000000   \n",
       "50%      0.037500           0.015698         1.000000       0.000000   \n",
       "75%      0.100000           0.017005         1.000000       0.000000   \n",
       "max      1.000000           1.000000         1.000000       1.000000   \n",
       "\n",
       "       exercised_stock_options    expenses  from_messages  \\\n",
       "count               144.000000  144.000000     144.000000   \n",
       "mean                  0.060434    0.154638       0.025305   \n",
       "std                   0.139614    0.198062       0.100966   \n",
       "min                   0.000000    0.000000       0.000000   \n",
       "25%                   0.000000    0.000000       0.000000   \n",
       "50%                   0.017710    0.088222       0.001218   \n",
       "75%                   0.049015    0.233116       0.003689   \n",
       "max                   1.000000    1.000000       1.000000   \n",
       "\n",
       "       from_poi_to_this_person  from_this_person_to_poi  loan_advances  \\\n",
       "count               144.000000               144.000000     144.000000   \n",
       "mean                  0.073403                 0.040435       0.007149   \n",
       "std                   0.140676                 0.130999       0.083342   \n",
       "min                   0.000000                 0.000000       0.000000   \n",
       "25%                   0.000000                 0.000000       0.000000   \n",
       "50%                   0.007576                 0.000000       0.000000   \n",
       "75%                   0.078125                 0.022989       0.000000   \n",
       "max                   1.000000                 1.000000       1.000000   \n",
       "\n",
       "             ...           rel_other  rel_expenses  rel_director_fees  \\\n",
       "count        ...          144.000000    144.000000         144.000000   \n",
       "mean         ...            0.108559      0.095527           0.008437   \n",
       "std          ...            0.221239      0.240176           0.083992   \n",
       "min          ...            0.000000      0.000000           0.000000   \n",
       "25%          ...            0.000000      0.000000           0.000000   \n",
       "50%          ...            0.000720      0.015768           0.000000   \n",
       "75%          ...            0.075646      0.055635           0.000000   \n",
       "max          ...            1.000000      1.000000           1.000000   \n",
       "\n",
       "       rel_deferred_income  rel_exercised_stock_options  rel_restricted_stock  \\\n",
       "count           144.000000                   144.000000            144.000000   \n",
       "mean              0.991324                     0.533666              0.115577   \n",
       "std               0.083976                     0.368718              0.135435   \n",
       "min               0.000000                     0.000000              0.000000   \n",
       "25%               0.999890                     0.069336              0.000000   \n",
       "50%               1.000000                     0.653732              0.081353   \n",
       "75%               1.000000                     0.860527              0.186282   \n",
       "max               1.000000                     1.000000              1.000000   \n",
       "\n",
       "       rel_restricted_stock_deferred  fraction_poi  fraction_to_poi  \\\n",
       "count                     144.000000    144.000000       144.000000   \n",
       "mean                        0.980331      0.127001         0.109922   \n",
       "std                         0.102345      0.190891         0.185935   \n",
       "min                         0.000000      0.000000         0.000000   \n",
       "25%                         1.000000      0.000000         0.000000   \n",
       "50%                         1.000000      0.039099         0.000000   \n",
       "75%                         1.000000      0.193167         0.198827   \n",
       "max                         1.000000      1.000000         1.000000   \n",
       "\n",
       "       fraction_from_poi  \n",
       "count         144.000000  \n",
       "mean            0.104317  \n",
       "std             0.167558  \n",
       "min             0.000000  \n",
       "25%             0.000000  \n",
       "50%             0.022782  \n",
       "75%             0.137655  \n",
       "max             1.000000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "## Q2-3: why do I need to select features?\n",
    "\n",
    "The goal of feature selection is to select best number of top features or reduce dimension of features. According to [a blog post by Jason Brownlee](http://machinelearningmastery.com/feature-selection-machine-learning-python/), having irrelevant features in the dataset can decrease the accuracy of many models.\n",
    "\n",
    "Three benefits of performing feature selection before modeling your data are:\n",
    "\n",
    "- Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "- Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "- Reduces Training Time: Less data means that algorithms train faster.\n",
    "\n",
    "\n",
    "## Q2-4: what selection process to use?\n",
    "\n",
    "1. Univariate Selection such as SelectKBest: statistical tests can be used to select the features that have the strongest relationship with the output variable. For the first trial, I will choose 7 or less features. The number 7 threshold came from the curve of dimensionality, where you may need exponentially more data points as you add more features, that is, 2^(n_featuers) = # of data points. I have 144 data points. 2^7 = 128, so 7 is the max feature number. Thus, I use **SelectKBest** process to pick 7 features.\n",
    "\n",
    "2. Dimensionality Reduction such as PCA: PCA (or Principal Component Analysis) uses linear algebra to transform the dataset into a compressed form. I think chosing 2-3 dimensions after PCA transformation could be good start.\n",
    "\n",
    "## Q2-5: which feature scores to compare and reasons for the choice of parameter values\n",
    "\n",
    "I choose **f_classif** scoring function over variances, chi2, and mutual_info_classif. \n",
    "\n",
    "- [Variance](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) can be useful for unsupervised classification. Since I have already labels, utilizing labels for scoring could be better than soley reling on x-variables. \n",
    "\n",
    "- The chi-square distribution arises in tests of hypotheses concerning the independence of two random variables and concerning whether a discrete random variable follows a specified distribution. The F-distribution arises in tests of hypotheses concerning whether or not two population variances are equal and concerning whether or not three or more population means are equal. In other words, chi-square is most appropriate for categorical data, whereas f-value can be used for continuous data [(read more)](https://discussions.udacity.com/t/f-classif-versus-chi2/245226).\n",
    "\n",
    "- [The mutual information (MI)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144L, 7L)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select 7 features that have highest ANOVA F-value with the factor by poi label\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "selector = SelectKBest(k=7)\n",
    "selected7 = selector.fit_transform(df_scaled[features_list], df_scaled['poi'])\n",
    "selected7.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features with F-value & p-value:\n",
      "1 ('exercised_stock_options', 25.097541528735491, 1.5945438463623382e-06)\n",
      "2 ('total_stock_value', 24.467654047526391, 2.1058066490127594e-06)\n",
      "3 ('bonus', 21.060001707536578, 9.7024743412322453e-06)\n",
      "4 ('salary', 18.575703268041778, 3.0337961075305315e-05)\n",
      "5 ('deferred_income', 11.595547659732164, 0.00085980314391924004)\n",
      "6 ('long_term_incentive', 10.072454529369448, 0.0018454351466116368)\n",
      "7 ('restricted_stock', 9.3467007910514379, 0.0026699611393240469)\n",
      "8 ('total_payments', 8.8667215371077805, 0.0034159213705928374)\n",
      "9 ('shared_receipt_with_poi', 8.7464855321290802, 0.0036344020243633686)\n",
      "10 ('loan_advances', 7.2427303965360172, 0.0079738162605691599)\n",
      "11 ('expenses', 6.234201140506757, 0.013673150875383932)\n",
      "12 ('from_poi_to_this_person', 5.3449415231473347, 0.022220727960811395)\n",
      "13 ('other', 4.2049708583014187, 0.042144700903259204)\n",
      "14 ('from_this_person_to_poi', 2.4265081272428799, 0.12152433983710857)\n",
      "15 ('director_fees', 2.1076559432760891, 0.14876949527311398)\n",
      "16 ('to_messages', 1.6988243485808538, 0.19455111487450777)\n",
      "17 ('deferral_payments', 0.21705893033950563, 0.64200389403828306)\n",
      "18 ('from_messages', 0.16416449823428589, 0.68596070789958996)\n",
      "19 ('restricted_stock_deferred', 0.064984311723709831, 0.7991535567029634)\n"
     ]
    }
   ],
   "source": [
    "scores = zip(features_list, selector.scores_, selector.pvalues_)\n",
    "sorted_scores = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "print\"features with F-value & p-value:\"\n",
    "n=0\n",
    "while (n < len(sorted_scores)):\n",
    "    print n+1, sorted_scores[n]\n",
    "    n +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exercised_stock_options', 'total_stock_value', 'bonus', 'salary', 'deferred_income', 'long_term_incentive', 'restricted_stock']\n"
     ]
    }
   ],
   "source": [
    "optimized_features_list = list(map(lambda x: x[0], sorted_scores))[0:7]\n",
    "print(optimized_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have noticed that some features (e.g. bonus, salary, and deferred_income) with \"NaN\" value disproportionally distributed between POI vs. non-POI groups show statistically strong relationship with labels. It is possible that the F-score was influenced by \"NaN\" driving biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3. Algorithm Search Planning \n",
    "\n",
    "# Validation Strategy\n",
    "\n",
    "## Q3-1: what is validation?\n",
    "Validation is an important process to asset the performance of a machine-learning algorithm. \n",
    "\n",
    "## Q3-2: what is a classic mistake you can make if you do it wrong? \n",
    "A classic mistake for my analysis is over-fitting. Learning the parameters of a prediction function and testing it on the same data is a methodological mistake, leading almost a perfect score, but it would fail to predict on unseen data. \n",
    "\n",
    "## Q3-3: how to validate algorithm analysis?  \n",
    "I think a proper validation method for the dataset with imbalanced classes is using cross validation iterators with stratification based on class labels, such as **StratifiedKFold** and **StratifiedShuffleSplit**. This would ensure that relative class frequencies is approximately preserved in each train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96L, 7L) (96L,)\n",
      "(48L, 7L) (48L,)\n"
     ]
    }
   ],
   "source": [
    "# generate a 3 train-test pairs iterator with test set size = 0.33\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=44)\n",
    "\n",
    "for train_index, test_index in skf.split(selected7, df_scaled['poi']):\n",
    "   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   X_train, X_test = selected7[train_index], selected7[test_index]\n",
    "   y_train, y_test = df_scaled['poi'][train_index], df_scaled['poi'][test_index]\n",
    "\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129L, 7L) (129L,)\n",
      "(15L, 7L) (15L,)\n"
     ]
    }
   ],
   "source": [
    "# generate a 1000 train-test pairs iterator with test set size = 0.1\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "#sss = StratifiedShuffleSplit(n_splits=1000, test_size=0.33, random_state=44)\n",
    "sss = StratifiedShuffleSplit(n_splits=1000, random_state=44)\n",
    "\n",
    "for train_index, test_index in sss.split(selected7, df_scaled['poi']):\n",
    "   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   X_train, X_test = selected7[train_index], selected7[test_index]\n",
    "   y_train, y_test = df_scaled['poi'][train_index], df_scaled['poi'][test_index]\n",
    "\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the project, project reviewers use tester.py. Thus, it is convenient for me to use the same validation method. The validation method used in tester.py is below.\n",
    "\n",
    ">cv = StratifiedShuffleSplit(labels, folds = 1000, random_state = 42)\n",
    "\n",
    "This is old version of StratifiedShuffleSplit which requires labels. sss with newer version of StratifiedShuffleSplit is equivalent to this.\n",
    "\n",
    ">sss = StratifiedShuffleSplit(n_splits=1000, random_state=44)\n",
    "\n",
    "Thus, I will stick with sss from now on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Exploration\n",
    "\n",
    "## Q3-4: what algorithms to begin? \n",
    "\n",
    "When dealing with small amounts of data, its reasonable to try as many algorithms as possible and to pick the best one since the cost of experimentation is low according to [blog post by Cheng-Tao Chu](http://ml.posthaven.com/machine-learning-done-wrong).\n",
    "- SVC\n",
    "- KNeighbors \n",
    "- Gaussian Naive Bayes\n",
    "- Decision Trees\n",
    "- Adaboost (boosted decision tree)\n",
    "- Random Forest\n",
    "\n",
    "## Research about algorithms and their parameters\n",
    "\n",
    "### 1. SVC Classifier\n",
    "\n",
    "According to [blog post by Cheng-Tao Chu](http://ml.posthaven.com/machine-learning-done-wrong), SVM is one of the most popular off-the-shelf modeling algorithms and one of its most powerful features is the ability to fit the model with different kernels. SVM kernels can be thought of as a way to automatically combine existing features to form a richer feature space. Since this power feature comes almost for free, most practitioners by default use kernel when training a SVM model. However, when the data has n<<p (number of samples << number of features) --  common in industries like medical data -- the richer feature space implies a much higher risk to overfit the data. In fact, high variance models should be avoided entirely when n<<p.\n",
    "\n",
    "According to [Edwin Chen](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/), High accuracy, nice theoretical guarantees regarding overfitting, and with an appropriate kernel they can work well even if youre data isnt linearly separable in the base feature space. Especially popular in the classification problems where very high-dimensional spaces are the norm. \n",
    "\n",
    "I think that SVC might be a good choice of classifier for this project, where we have multivariate features that could have non-linear interactions among features. \n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- C (penalty): 'clf__C': [0.1, 1, 10, 100, 1000]\n",
    "- kernal: 'clf__kernel': ['rbf', 'linear', 'poly'],\n",
    "- gamma (kernal coefficient): 'clf__gamma': [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "- tol (Tolerance for stopping criterion): 'clf__tol': [1e-3, 1e-4, 1e-5]\n",
    "- class_weight: 'clf__class_weight': ['balanced', None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86666666666666692"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit into SVC classifier and get accuracy using cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "\n",
    "scores = cross_val_score(svc, selected7, df_scaled['poi'], cv=sss)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GaussianNB Classifier\n",
    "\n",
    "According to [Edwin Chen](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/), super simple, youre just doing a bunch of counts. If the NB conditional independence assumption actually holds, a Naive Bayes classifier will converge quicker than discriminative models like logistic regression, so you need less training data. And even if the NB assumption doesnt hold, a NB classifier still often does a great job in practice. A good bet if want something fast and easy that performs pretty well. Its main disadvantage is that it cant learn interactions between features.\n",
    "\n",
    "According to [sklearn documentation](http://scikit-learn.org/stable/modules/naive_bayes.html), NB learners and classifiers can be extremely fast. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality. On the flip side, it is known to be a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "\n",
    "I think that NB might be a good choice of classifier for this project, where we have limited number of observations (total = 144) and even smaller training set for cross-validation. NB is high bias and low variance classifier, which has an advantage over low bias and high variance classifiers like KNeighbors, since the  latter will overfit.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- priors (Prior probabilities of the classes): default = None; meaning the priors are adjusted according to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86100000000000021"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "scores = cross_val_score(gnb, selected7, df_scaled['poi'], cv=sss)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. KNeighbors Classifier\n",
    "\n",
    "According to [sklearn documentation](http://scikit-learn.org/stable/modules/neighbors.html#classification), the principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). Despite its simplicity, nearest neighbors have been successful in a large number of classification and regression problems. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n",
    "\n",
    "I think that KNeighbors can be a risky option since it tends to overfit for small set of data. But it could be a robust method to handle outliers due to its non-parametricity.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- n_neighbors : 'clf__n_neighbors': [5, 8, 10, 15]\n",
    "- weights : 'clf__weights' : ['uniform','distance']\n",
    "- algorithm : 'clf__algorithm' : [auto, ball_tree, kd_tree, brute]\n",
    "- metric [(distance metric)](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html) : 'clf__metric' : ['euclidean', 'manhattan', 'minkowski']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85613333333333363"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier()\n",
    "\n",
    "scores = cross_val_score(neigh, selected7, df_scaled['poi'], cv=sss)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. DecisionTree Classifier\n",
    "\n",
    "According to [Edwin Chen](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/), DecisionTree (DT) is easy to interpret and explain. They easily handle feature interactions and theyre non-parametric, so you dont have to worry about outliers or whether the data is linearly separable (e.g., decision trees easily take care of cases where you have class A at the low end of some feature x, class B in the mid-range of feature x, and A again at the high end). One disadvantage is that they dont support online learning, so you have to rebuild your tree when new examples come on. Another disadvantage is that they easily overfit. According to [sklearn documentation](http://scikit-learn.org/stable/modules/tree.html), mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "\n",
    "I think that DT can be a risky classifier, not only because of overfitting, but also because of high tendency of bias for a dataset with imbalanced classes.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- criterion (measurement for split quality): 'clf__n_estimators': [gini, entropy]\n",
    "- splitter: 'clf__splitter': [best, random]\n",
    "- max_features : 'clf__max_features': [0.5, auto, log2, None]\n",
    "- max_depth : 'clf__max_depth': [3, 5, 10, None]\n",
    "- min_samples_leaf : 'clf__min_samples_leaf': [5, 4, 3, 2, 1]\n",
    "- class_weight: 'clf__class_weight': [balanced_subsample, balanced, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80040000000000011"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "scores = cross_val_score(dt, selected7, df_scaled['poi'], cv=sss)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RandomForest Classifier\n",
    "\n",
    "According to [sklearn documentation](http://scikit-learn.org/stable/modules/tree.html), DT can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble. Another disadvantage is that DT easily overfit, but thats where ensemble methods like random forests (or boosted trees) come in. Plus, random forests (RF) are often the winner for lots of problems in classification (usually slightly ahead of SVMs), theyre scalable, and you dont have to worry about tuning a bunch of parameters like you do with SVMs, so they seem to be quite popular these days\n",
    "\n",
    "I think that RF can be good alternative classifier to DT. According to [sklearn documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest), as a result of the randomness, the bias of the forest usually slightly increases with respect to the bias of a single non-random tree but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.\n",
    "\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- n_estimators (the number of trees in the forest) : 'clf__n_estimators': [10, 50, 100, 200]\n",
    "- criterion (measurement for split quality): 'clf__criterion': [gini, entropy]\n",
    "- max_features : 'clf__max_features': [0.5, auto, log2, None]\n",
    "- max_depth : 'clf__max_depth': [3, 5, 10, None]\n",
    "- min_samples_leaf : 'clf__min_samples_leaf': [5, 4, 3, 2, 1]\n",
    "- class_weight: 'clf__class_weight': [balanced, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85613333333333363"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rdf = RandomForestClassifier()\n",
    "\n",
    "scores = cross_val_score(rdf, selected7, df_scaled['poi'], cv=sss)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. AdaBoost Classifier\n",
    "\n",
    "The core principle of AdaBoost (AB) is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. According to [blog post by Cheng-Tao Chu](http://ml.posthaven.com/machine-learning-done-wrong), some models are more sensitive to outliers than others. For instance, AdaBoost might treat those outliers as \"hard\" cases and put tremendous weights on outliers, while DT might simply count each outlier as one false classification. If the data set contains a fair number of outliers, it's important to either use modeling algorithm robust against outliers or filter the outliers out.\n",
    "\n",
    "AB will run slow like RF because of iteration steps but the advantage of both classifiers is high predictive accuracy. I think it is worth trying because we have relatively small set of data.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- base_estimator: default=DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8212666666666667"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adb = AdaBoostClassifier()\n",
    "\n",
    "scores = cross_val_score(adb, selected7, df_scaled['poi'], cv=sss)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics Usage\n",
    "\n",
    "## Q3-5:give at least 2 evaluation metrics and the average performance for each of them.\n",
    "\n",
    "- accuracy: correct label (predicted label == true label)/total testing data points\n",
    "- precision: true POI/(true POI + false non-POI)\n",
    "- recall: true POI/(true POI + false POI)\n",
    "- average_precision: the area under the precision-recall curve\n",
    "- f1: 2 * (precision * recall) / (precision + recall)\n",
    "- f1_weighted: Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters macro to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "[Evaluate multiple scores on sklearn cross_val_score](https://stackoverflow.com/questions/35876508/evaluate-multiple-scores-on-sklearn-cross-val-score) for code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.866666666667\n",
      "precision : 0.0\n",
      "recall : 0.0\n",
      "average_precision : 0.38899757881\n",
      "f1 : 0.0\n",
      "f1_weighted : 0.804761904762\n"
     ]
    }
   ],
   "source": [
    "# compare evaluating metrics on SVC \n",
    "scorer = [\"accuracy\", \"precision\", \"recall\", \"average_precision\", \"f1\", \"f1_weighted\"]\n",
    "def print_scores(clf):\n",
    "    for score in scorer:\n",
    "        m_score = cross_val_score(clf, selected7, df_scaled['poi'], cv=sss, \\\n",
    "                        scoring=score).mean()\n",
    "        print score, ':', m_score\n",
    "\n",
    "print_scores(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.861\n",
      "precision : 0.429035714286\n",
      "recall : 0.3995\n",
      "average_precision : 0.483781832751\n",
      "f1 : 0.389465873016\n",
      "f1_weighted : 0.8492964868\n"
     ]
    }
   ],
   "source": [
    "# compare evaluating metrics on GaussianNB\n",
    "print_scores(gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Tuning\n",
    "\n",
    "## Q3-6: what does it mean to tune the parameters of an algorithm, and what can happen if you dont do this well?  \n",
    "\n",
    "The machine learning algorithms are parameterized so that their behavior can be tuned for a given problem. It's important to perform parameter tuning here to adjust the precision and recall. \n",
    "\n",
    "Parameters tuning refers to the adjustment of the algorithm when training, in order to improve the fit on the test set. Parameter can influence the outcome of the learning process, the more tuned the parameters, the more biased the algorithm will be to the training data & test harness. The strategy can be effective but it can also lead to more fragile models & overfit the test harness but don't perform well in practice\n",
    "\n",
    "## Q3-7: How to tune the parameters of your particular algorithm? \n",
    "\n",
    "I can use automated parameter search processes, such as **GridSearchCV** and **RandomizedSearchCV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape=None, degree=4, gamma=0.1, kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune parameters of SVC using GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = svc\n",
    "parameters = {'kernel': ['rbf', 'linear', 'poly'], \\\n",
    "              'C': [0.1, 1, 10, 100, 1000],\\\n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \\\n",
    "              'degree': [3, 4, 5], \\\n",
    "              'class_weight':['balanced', None]}\n",
    "\n",
    "grid_search = GridSearchCV(clf, parameters)\n",
    "grid_result = grid_search.fit(selected7, df_scaled['poi']).best_estimator_\n",
    "grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.133333333333\n",
      "precision : 0.133333333333\n",
      "recall : 1.0\n",
      "average_precision : 0.4336371448\n",
      "f1 : 0.235294117647\n",
      "f1_weighted : 0.0313725490196\n"
     ]
    }
   ],
   "source": [
    "# compare evaluating metrics on best estimator from grid search cv\n",
    "print_scores(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=88.085553333754788, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.064289877113293634,\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune parameters of SVC using RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = {'kernel': ['rbf', 'linear', 'poly'], \\\n",
    "              'C': scipy.stats.expon(scale=100), \\\n",
    "              'gamma': scipy.stats.expon(scale=.1), \\\n",
    "              'class_weight':['balanced', None]}\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, parameters, n_iter=20)\n",
    "start = time()\n",
    "random_result = random_search.fit(selected7, df_scaled['poi']).best_estimator_\n",
    "random_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.867533333333\n",
      "precision : 0.013\n",
      "recall : 0.0065\n",
      "average_precision : 0.421023055417\n",
      "f1 : 0.00866666666667\n",
      "f1_weighted : 0.806304938272\n"
     ]
    }
   ],
   "source": [
    "# compare evaluating metrics on best estimator from randomized search cv\n",
    "print_scores(random_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part4. Algorithm Search\n",
    "\n",
    "Deciding the number of features (e.g. k=7 for SelectKBest) is somewhat arbitrary and it can be dependent on classifier algorithms; different algorithms have different optimized number of features. So, instead of deciding a rigid number, I will use pipeline to optimize the number of features according to a choice of classifier.\n",
    "\n",
    "- Approach1: Select features and Optimize parameters of classifier\n",
    "- Approach2: Reduce feature dimensions and Optimize parameters of classifier\n",
    "\n",
    "## Pipeline Approach1\n",
    "\n",
    ">approach1 = Pipeline([('selector', SelectKBest()), ('clf', classifier)])\n",
    "\n",
    "Construct steps for optimizing the number of features using univariate selection method and the parameters of classifier simultaneously using Pipeline. \n",
    "\n",
    ">grid_search = GridSearchCV(approach1, parameters[classifier], scoring='f1')\n",
    "\n",
    "Select k number of features and find best estimator with highest f-values using GridSearchCV.\n",
    "\n",
    ">tester.dump_classifier_and_data(new_clf, new_dataset, new_list)\n",
    ">tester.main()\n",
    "\n",
    "Finally, get new_clf, new_dataset, and new_list from the grid_search results and plug the optimized results into tester to get evaluating metrics from cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a procedue to take feature list and result from pipeline grid search\n",
    "# and return cross-validation evalutating metrics using tester.py module\n",
    "def performance(old_list, grid_result):\n",
    "    selector = gird_result.named_steps['selector']\n",
    "    k_features = gird_result.named_steps['selector'].get_params(deep=True)['k']\n",
    "    print \"Number of features selected: %i\" %(k_features)\n",
    "    selected = selector.fit_transform(df_scaled[old_list], df_scaled['poi'])\n",
    "    scores = zip(old_list, selector.scores_, selector.pvalues_)\n",
    "    sorted_scores = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "    new_list = list(map(lambda x: x[0], sorted_scores))[0:k_features]\n",
    "    new_list = ['poi']+ new_list\n",
    "    new_dataset = df_scaled[new_list].to_dict(orient = 'index')  \n",
    "    new_clf = gird_result.named_steps['clf']\n",
    "    tester.dump_classifier_and_data(new_clf, new_dataset, new_list)\n",
    "    tester.main()\n",
    "    print \"\\nThis took %.2f seconds\\n\" %(time() - start)\n",
    "    print \"--------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 9 pairs of Classifier-FeatureList\n",
    "- Pipeline: Approach1 \n",
    "- Classifiers: SVC, GaussianNB, and KNeighbors\n",
    "- FeatureLists:  1. features_list, 2. total_features, 3. rel_total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of features selected: 19\n",
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.77407\tPrecision: 0.30660\tRecall: 0.55050\tF1: 0.39385\tF2: 0.47494\n",
      "\tTotal predictions: 15000\tTrue positives: 1101\tFalse positives: 2490\tFalse negatives:  899\tTrue negatives: 10510\n",
      "\n",
      "\n",
      "This took 108.90 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of features selected: 10\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.83753\tPrecision: 0.36829\tRecall: 0.30550\tF1: 0.33397\tF2: 0.31629\n",
      "\tTotal predictions: 15000\tTrue positives:  611\tFalse positives: 1048\tFalse negatives: 1389\tTrue negatives: 11952\n",
      "\n",
      "\n",
      "This took 1.04 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of features selected: 19\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "\tAccuracy: 0.86133\tPrecision: 0.34848\tRecall: 0.04600\tF1: 0.08127\tF2: 0.05566\n",
      "\tTotal predictions: 15000\tTrue positives:   92\tFalse positives:  172\tFalse negatives: 1908\tTrue negatives: 12828\n",
      "\n",
      "\n",
      "This took 13.92 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "========================================================\n",
      "2\n",
      "Number of features selected: 10\n",
      "SVC(C=1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.70113\tPrecision: 0.26312\tRecall: 0.68950\tF1: 0.38089\tF2: 0.52073\n",
      "\tTotal predictions: 15000\tTrue positives: 1379\tFalse positives: 3862\tFalse negatives:  621\tTrue negatives: 9138\n",
      "\n",
      "\n",
      "This took 77.53 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of features selected: 10\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.83973\tPrecision: 0.37743\tRecall: 0.31100\tF1: 0.34101\tF2: 0.32235\n",
      "\tTotal predictions: 15000\tTrue positives:  622\tFalse positives: 1026\tFalse negatives: 1378\tTrue negatives: 11974\n",
      "\n",
      "\n",
      "This took 1.28 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of features selected: 19\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "\tAccuracy: 0.86800\tPrecision: 0.52778\tRecall: 0.09500\tF1: 0.16102\tF2: 0.11364\n",
      "\tTotal predictions: 15000\tTrue positives:  190\tFalse positives:  170\tFalse negatives: 1810\tTrue negatives: 12830\n",
      "\n",
      "\n",
      "This took 14.12 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "========================================================\n",
      "3\n",
      "Number of features selected: 7\n",
      "SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.85409\tPrecision: 0.59500\tRecall: 0.61850\tF1: 0.60652\tF2: 0.61365\n",
      "\tTotal predictions: 11000\tTrue positives: 1237\tFalse positives:  842\tFalse negatives:  763\tTrue negatives: 8158\n",
      "\n",
      "\n",
      "This took 80.02 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of features selected: 15\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.46860\tPrecision: 0.14756\tRecall: 0.62500\tF1: 0.23875\tF2: 0.37945\n",
      "\tTotal predictions: 15000\tTrue positives: 1250\tFalse positives: 7221\tFalse negatives:  750\tTrue negatives: 5779\n",
      "\n",
      "\n",
      "This took 1.09 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of features selected: 7\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n",
      "\tAccuracy: 0.81200\tPrecision: 0.39881\tRecall: 0.06700\tF1: 0.11473\tF2: 0.08037\n",
      "\tTotal predictions: 11000\tTrue positives:  134\tFalse positives:  202\tFalse negatives: 1866\tTrue negatives: 8798\n",
      "\n",
      "\n",
      "This took 15.75 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "# build pipeline with selector and clf steps\n",
    "# and iterate 3 classifiers (svc, gnb, and neigh) with their parameter sets\n",
    "# and iterate 3 feature lists:  1. features_list, 2. total_features, 3. rel_total_features\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# declare paremeters grid\n",
    "parameters = {svc: {'selector__k':[19, 15, 10, 7], \\\n",
    "                     'clf__kernel': ['rbf', 'linear', 'poly'], \\\n",
    "                     'clf__C': [0.1, 1, 10, 100, 1000], \\\n",
    "                     'clf__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \\\n",
    "                     'clf__class_weight': ['balanced', None]}, \\\n",
    "              gnb: {'selector__k':[19, 15, 10, 7]}, \\\n",
    "              neigh: {'selector__k':[19, 15, 10, 7], \\\n",
    "                      'clf__n_neighbors': [5, 8, 10, 15], \\\n",
    "                      'clf__weights' : ['uniform','distance'], \\\n",
    "                      'clf__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'], \\\n",
    "                      'clf__metric' : ['euclidean', 'manhattan', 'minkowski']}}\n",
    "\n",
    "num = 1\n",
    "for features in [features_list, total_features, rel_total_features]:\n",
    "    print num\n",
    "    for classifier in parameters:\n",
    "        approach1 = Pipeline([('selector', SelectKBest()), \\\n",
    "                      ('clf', classifier)])\n",
    "        grid_search = GridSearchCV(approach1, parameters[classifier], scoring='f1')\n",
    "        start = time()\n",
    "        gird_result = grid_search.fit(df_scaled[features], df_scaled['poi']).best_estimator_\n",
    "        performance(features, gird_result)\n",
    "    print \"========================================================\"\n",
    "    num += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test DecisionTree with 3 FeatureLists\n",
    "- Pipeline: Approach1 \n",
    "- Classifier: DecisionTree\n",
    "- FeatureLists:  1. features_list, 2. total_features, 3. rel_total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of features selected: 7\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='random')\n",
      "\tAccuracy: 0.72360\tPrecision: 0.23493\tRecall: 0.47550\tF1: 0.31448\tF2: 0.39467\n",
      "\tTotal predictions: 15000\tTrue positives:  951\tFalse positives: 3097\tFalse negatives: 1049\tTrue negatives: 9903\n",
      "\n",
      "\n",
      "This took 71.90 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "2\n",
      "Number of features selected: 19\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=5,\n",
      "            max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=4,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.77820\tPrecision: 0.31787\tRecall: 0.57900\tF1: 0.41042\tF2: 0.49729\n",
      "\tTotal predictions: 15000\tTrue positives: 1158\tFalse positives: 2485\tFalse negatives:  842\tTrue negatives: 10515\n",
      "\n",
      "\n",
      "This took 72.44 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "3\n",
      "Number of features selected: 15\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.83780\tPrecision: 0.33511\tRecall: 0.22000\tF1: 0.26562\tF2: 0.23623\n",
      "\tTotal predictions: 15000\tTrue positives:  440\tFalse positives:  873\tFalse negatives: 1560\tTrue negatives: 12127\n",
      "\n",
      "\n",
      "This took 73.66 seconds\n",
      "\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "approach1 = Pipeline([('selector', SelectKBest()), \\\n",
    "                      ('clf', dt)])\n",
    "\n",
    "parameters = {'selector__k':[19, 15, 10, 7], \\\n",
    "              'clf__criterion': ['gini', 'entropy'], \\\n",
    "              'clf__splitter': ['best', 'random'], \\\n",
    "              'clf__max_features': [0.5, 'auto', 'log2', None], \\\n",
    "              'clf__max_depth': [3, 5, 10, None], \\\n",
    "              'clf__min_samples_leaf': [5, 4, 3, 2, 1], \\\n",
    "              'clf__class_weight': ['balanced', None]}\n",
    "\n",
    "num = 1\n",
    "for features in [features_list, total_features, rel_total_features]:\n",
    "    print num\n",
    "    grid_search = GridSearchCV(approach1, parameters, scoring='f1')\n",
    "    start = time()\n",
    "    gird_result = grid_search.fit(df_scaled[features], df_scaled['poi']).best_estimator_\n",
    "    performance(features, gird_result)\n",
    "    num += 1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test RandomForest with 3 FeatureLists\n",
    "- Pipeline: Approach1 \n",
    "- Classifier: RandomForest\n",
    "- FeatureLists:  1. features_list, 2. total_features, 3. rel_total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of features selected: 15\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.85547\tPrecision: 0.36751\tRecall: 0.11650\tF1: 0.17692\tF2: 0.13493\n",
      "\tTotal predictions: 15000\tTrue positives:  233\tFalse positives:  401\tFalse negatives: 1767\tTrue negatives: 12599\n",
      "\n",
      "\n",
      "This took 36.98 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "2\n",
      "Number of features selected: 7\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.86247\tPrecision: 0.45794\tRecall: 0.17150\tF1: 0.24955\tF2: 0.19602\n",
      "\tTotal predictions: 15000\tTrue positives:  343\tFalse positives:  406\tFalse negatives: 1657\tTrue negatives: 12594\n",
      "\n",
      "\n",
      "This took 37.75 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "3\n",
      "Number of features selected: 10\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.81358\tPrecision: 0.30478\tRecall: 0.09250\tF1: 0.14193\tF2: 0.10747\n",
      "\tTotal predictions: 12000\tTrue positives:  185\tFalse positives:  422\tFalse negatives: 1815\tTrue negatives: 9578\n",
      "\n",
      "\n",
      "This took 37.32 seconds\n",
      "\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "approach1 = Pipeline([('selector', SelectKBest()), \\\n",
    "                      ('clf', rdf)])\n",
    "\n",
    "parameters = {'selector__k':[19, 15, 10, 7]}\n",
    "\n",
    "num = 1\n",
    "for features in [features_list, total_features, rel_total_features]:\n",
    "    print num\n",
    "    grid_search = GridSearchCV(approach1, parameters, scoring='f1')\n",
    "    start = time()\n",
    "    gird_result = grid_search.fit(df_scaled[features], df_scaled['poi']).best_estimator_\n",
    "    performance(features, gird_result)\n",
    "    num += 1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AdaBoost with 3 FeatureLists\n",
    "- Pipeline: Approach1 \n",
    "- Classifier: AdaBoost\n",
    "- FeatureLists:  1. features_list, 2. total_features, 3. rel_total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of features selected: 15\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.85040\tPrecision: 0.41528\tRecall: 0.29900\tF1: 0.34767\tF2: 0.31674\n",
      "\tTotal predictions: 15000\tTrue positives:  598\tFalse positives:  842\tFalse negatives: 1402\tTrue negatives: 12158\n",
      "\n",
      "\n",
      "This took 118.86 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "2\n",
      "Number of features selected: 7\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.82953\tPrecision: 0.33491\tRecall: 0.28250\tF1: 0.30648\tF2: 0.29163\n",
      "\tTotal predictions: 15000\tTrue positives:  565\tFalse positives: 1122\tFalse negatives: 1435\tTrue negatives: 11878\n",
      "\n",
      "\n",
      "This took 108.43 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "3\n",
      "Number of features selected: 7\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.77118\tPrecision: 0.30579\tRecall: 0.20350\tF1: 0.24437\tF2: 0.21809\n",
      "\tTotal predictions: 11000\tTrue positives:  407\tFalse positives:  924\tFalse negatives: 1593\tTrue negatives: 8076\n",
      "\n",
      "\n",
      "This took 104.48 seconds\n",
      "\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "approach1 = Pipeline([('selector', SelectKBest()), \\\n",
    "                      ('clf', adb)])\n",
    "\n",
    "parameters = {'selector__k':[19, 15, 10, 7]}\n",
    "  \n",
    "num = 1\n",
    "for features in [features_list, total_features, rel_total_features]:\n",
    "    print num\n",
    "    grid_search = GridSearchCV(approach1, parameters, scoring='f1')\n",
    "    start = time()\n",
    "    gird_result = grid_search.fit(df_scaled[features], df_scaled['poi']).best_estimator_\n",
    "    performance(features, gird_result)\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Approach2\n",
    "\n",
    ">approach2 = Pipeline([('reducer', PCA()), ('clf', classifier)])\n",
    "\n",
    "Construct steps for optimizing the number of principal components using dimensionality reduction method (PCA) and the parameters of classifier simultaneously using Pipeline. \n",
    "\n",
    ">grid_search = GridSearchCV(approach2, parameters[classifier], scoring='f1')\n",
    "\n",
    "Select n number of components and find best estimator with highest f-values using GridSearchCV.\n",
    "\n",
    ">tester.dump_classifier_and_data(new_clf, new_dataset, new_list)\n",
    ">tester.main()\n",
    "\n",
    "Finally, get new_clf, new_dataset (pca tranformed data), and new_list (pca dimension) from the grid_search results and plug the optimized results into tester to get evaluating metrics from cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 9 pairs of Classifier-FeatureList\n",
    "- Pipeline: Approach2\n",
    "- Classifiers: SVC, GaussianNB, and KNeighbors\n",
    "- FeatureLists:  1. features_list, 2. total_features, 3. rel_total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a procedue to take feature list and result from pipeline grid search\n",
    "# and return cross-validation evalutating metrics using tester.py module\n",
    "def performance_w_pca(old_list, grid_result):\n",
    "    reducer = gird_result.named_steps['reducer']\n",
    "    n_components = gird_result.named_steps['reducer'].get_params(deep=True)['n_components']\n",
    "    print \"Number of component: %i\" %(n_components)\n",
    "    reduced = pd.DataFrame(reducer.fit_transform(df_scaled[old_list]), index=df_scaled.index)\n",
    "    new_list = list(reduced.columns)\n",
    "    new_list = ['poi']+ new_list\n",
    "    reduced.insert(0, 'poi', df_scaled.poi)\n",
    "    new_dataset = reduced.to_dict(orient = 'index') \n",
    "    new_clf = gird_result.named_steps['clf']\n",
    "    tester.dump_classifier_and_data(new_clf, new_dataset, new_list)\n",
    "    tester.main()\n",
    "    print \"\\nThis took %.2f seconds\\n\" %(time() - start)\n",
    "    print \"--------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of component: 1\n",
      "SVC(C=10, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.73587\tPrecision: 0.31178\tRecall: 0.81250\tF1: 0.45064\tF2: 0.61497\n",
      "\tTotal predictions: 15000\tTrue positives: 1625\tFalse positives: 3587\tFalse negatives:  375\tTrue negatives: 9413\n",
      "\n",
      "\n",
      "This took 31.94 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of component: 7\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.83107\tPrecision: 0.38259\tRecall: 0.43500\tF1: 0.40711\tF2: 0.42340\n",
      "\tTotal predictions: 15000\tTrue positives:  870\tFalse positives: 1404\tFalse negatives: 1130\tTrue negatives: 11596\n",
      "\n",
      "\n",
      "This took 1.15 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of component: 1\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "\tAccuracy: 0.84673\tPrecision: 0.02540\tRecall: 0.00400\tF1: 0.00691\tF2: 0.00481\n",
      "\tTotal predictions: 15000\tTrue positives:    8\tFalse positives:  307\tFalse negatives: 1992\tTrue negatives: 12693\n",
      "\n",
      "\n",
      "This took 21.49 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "========================================================\n",
      "2\n",
      "Number of component: 1\n",
      "SVC(C=10, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.71113\tPrecision: 0.28069\tRecall: 0.74650\tF1: 0.40798\tF2: 0.56048\n",
      "\tTotal predictions: 15000\tTrue positives: 1493\tFalse positives: 3826\tFalse negatives:  507\tTrue negatives: 9174\n",
      "\n",
      "\n",
      "This took 32.28 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of component: 10\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.81920\tPrecision: 0.34979\tRecall: 0.41450\tF1: 0.37941\tF2: 0.39971\n",
      "\tTotal predictions: 15000\tTrue positives:  829\tFalse positives: 1541\tFalse negatives: 1171\tTrue negatives: 11459\n",
      "\n",
      "\n",
      "This took 1.41 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of component: 2\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n",
      "\tAccuracy: 0.86793\tPrecision: 0.51252\tRecall: 0.19450\tF1: 0.28199\tF2: 0.22206\n",
      "\tTotal predictions: 15000\tTrue positives:  389\tFalse positives:  370\tFalse negatives: 1611\tTrue negatives: 12630\n",
      "\n",
      "\n",
      "This took 20.68 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "========================================================\n",
      "3\n",
      "Number of component: 3\n",
      "SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.74140\tPrecision: 0.30963\tRecall: 0.76400\tF1: 0.44066\tF2: 0.59065\n",
      "\tTotal predictions: 15000\tTrue positives: 1528\tFalse positives: 3407\tFalse negatives:  472\tTrue negatives: 9593\n",
      "\n",
      "\n",
      "This took 31.36 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of component: 10\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.75407\tPrecision: 0.18192\tRecall: 0.24150\tF1: 0.20752\tF2: 0.22665\n",
      "\tTotal predictions: 15000\tTrue positives:  483\tFalse positives: 2172\tFalse negatives: 1517\tTrue negatives: 10828\n",
      "\n",
      "\n",
      "This took 1.15 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of component: 2\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n",
      "           weights='distance')\n",
      "\tAccuracy: 0.84720\tPrecision: 0.02903\tRecall: 0.00450\tF1: 0.00779\tF2: 0.00542\n",
      "\tTotal predictions: 15000\tTrue positives:    9\tFalse positives:  301\tFalse negatives: 1991\tTrue negatives: 12699\n",
      "\n",
      "\n",
      "This took 21.79 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "# build pipeline with reducer and clf steps\n",
    "# and iterate 3 classifiers (svc, gnb, and neigh) with their parameter sets\n",
    "# and iterate 3 feature lists:  1. features_list, 2. total_features, 3. rel_total_features\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "parameters = {svc: {'reducer__n_components':[1, 2, 3, 5, 7, 10], \\\n",
    "                    'clf__kernel': ['rbf', 'linear', 'poly'], \\\n",
    "                    'clf__C': [0.1, 1, 10, 100, 1000], \\\n",
    "                    'clf__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \\\n",
    "                    'clf__class_weight': ['balanced', None]}, \\\n",
    "              gnb: {'reducer__n_components':[1, 2, 3, 5, 7, 10]}, \\\n",
    "              neigh: {'reducer__n_components':[1, 2, 3, 5, 7, 10], \\\n",
    "                      'clf__n_neighbors': [5, 8, 10, 15], \\\n",
    "                      'clf__weights' : ['uniform','distance'], \\\n",
    "                      'clf__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'], \\\n",
    "                      'clf__metric' : ['euclidean', 'manhattan', 'minkowski']}}\n",
    "\n",
    "num = 1\n",
    "for features in [features_list, total_features, rel_total_features]:\n",
    "    print num\n",
    "    for classifier in parameters:\n",
    "        approach2 = Pipeline([('reducer', PCA()), \\\n",
    "                      ('clf', classifier)])\n",
    "        grid_search = GridSearchCV(approach2, parameters[classifier], scoring='f1')\n",
    "        start = time()\n",
    "        gird_result = grid_search.fit(df_scaled[features], df_scaled['poi']).best_estimator_\n",
    "        performance_w_pca(features, gird_result)\n",
    "    print \"========================================================\"\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test DecisionTree with 3 FeatureLists\n",
    "- Pipeline: Approach2 \n",
    "- Classifier: DecisionTree\n",
    "- FeatureLists:  1. features_list, 2. total_features, 3. rel_total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of component: 3\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='random')\n",
      "\tAccuracy: 0.68093\tPrecision: 0.23992\tRecall: 0.64250\tF1: 0.34937\tF2: 0.48106\n",
      "\tTotal predictions: 15000\tTrue positives: 1285\tFalse positives: 4071\tFalse negatives:  715\tTrue negatives: 8929\n",
      "\n",
      "\n",
      "This took 124.41 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "2\n",
      "Number of component: 10\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
      "            max_depth=None, max_features=0.5, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='random')\n",
      "\tAccuracy: 0.80047\tPrecision: 0.23321\tRecall: 0.21700\tF1: 0.22481\tF2: 0.22006\n",
      "\tTotal predictions: 15000\tTrue positives:  434\tFalse positives: 1427\tFalse negatives: 1566\tTrue negatives: 11573\n",
      "\n",
      "\n",
      "This took 125.89 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "3\n",
      "Number of component: 7\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=3,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.82360\tPrecision: 0.25751\tRecall: 0.17150\tF1: 0.20588\tF2: 0.18378\n",
      "\tTotal predictions: 15000\tTrue positives:  343\tFalse positives:  989\tFalse negatives: 1657\tTrue negatives: 12011\n",
      "\n",
      "\n",
      "This took 117.21 seconds\n",
      "\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "approach2 = Pipeline([('reducer', PCA()), ('clf', dt)])\n",
    "\n",
    "parameters = {'reducer__n_components':[1, 2, 3, 5, 7, 10], \\\n",
    "              'clf__criterion': ['gini', 'entropy'], \\\n",
    "              'clf__splitter': ['best', 'random'], \\\n",
    "              'clf__max_features': [0.5, 'auto', 'log2', None], \\\n",
    "              'clf__max_depth': [3, 5, 10, None], \\\n",
    "              'clf__min_samples_leaf': [5, 4, 3, 2, 1], \\\n",
    "              'clf__class_weight': ['balanced', None]}\n",
    "\n",
    "num = 1\n",
    "for features in [features_list, total_features, rel_total_features]:\n",
    "    print num\n",
    "    grid_search = GridSearchCV(approach2, parameters, scoring='f1')\n",
    "    start = time()\n",
    "    gird_result = grid_search.fit(df_scaled[features], df_scaled['poi']).best_estimator_\n",
    "    performance_w_pca(features, gird_result)\n",
    "    num += 1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test RandomForest with 3 FeatureLists\n",
    "- Pipeline: Approach2\n",
    "- Classifier: RandomForest\n",
    "- FeatureLists:  1. features_list, 2. total_features, 3. rel_total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of component: 1\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.82613\tPrecision: 0.28889\tRecall: 0.20800\tF1: 0.24186\tF2: 0.22034\n",
      "\tTotal predictions: 15000\tTrue positives:  416\tFalse positives: 1024\tFalse negatives: 1584\tTrue negatives: 11976\n",
      "\n",
      "\n",
      "This took 35.37 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "2\n",
      "Number of component: 1\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.82327\tPrecision: 0.29845\tRecall: 0.24100\tF1: 0.26667\tF2: 0.25065\n",
      "\tTotal predictions: 15000\tTrue positives:  482\tFalse positives: 1133\tFalse negatives: 1518\tTrue negatives: 11867\n",
      "\n",
      "\n",
      "This took 34.91 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "3\n",
      "Number of component: 10\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.84147\tPrecision: 0.11429\tRecall: 0.02800\tF1: 0.04498\tF2: 0.03298\n",
      "\tTotal predictions: 15000\tTrue positives:   56\tFalse positives:  434\tFalse negatives: 1944\tTrue negatives: 12566\n",
      "\n",
      "\n",
      "This took 35.96 seconds\n",
      "\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "approach2 = Pipeline([('reducer', PCA()),('clf', rdf)])\n",
    "\n",
    "parameters = {'reducer__n_components':[1, 2, 3, 5, 7, 10]}\n",
    "\n",
    "num = 1\n",
    "for features in [features_list, total_features, rel_total_features]:\n",
    "    print num\n",
    "    grid_search = GridSearchCV(approach2, parameters, scoring='f1')\n",
    "    start = time()\n",
    "    gird_result = grid_search.fit(df_scaled[features], df_scaled['poi']).best_estimator_\n",
    "    performance_w_pca(features, gird_result)\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AdaBoost with 3 FeatureLists\n",
    "- Pipeline: Approach2\n",
    "- Classifier: AdaBoost\n",
    "- FeatureLists:  1. features_list, 2. total_features, 3. rel_total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of component: 1\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.82207\tPrecision: 0.26592\tRecall: 0.19000\tF1: 0.22164\tF2: 0.20151\n",
      "\tTotal predictions: 15000\tTrue positives:  380\tFalse positives: 1049\tFalse negatives: 1620\tTrue negatives: 11951\n",
      "\n",
      "\n",
      "This took 104.41 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "2\n",
      "Number of component: 3\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.80947\tPrecision: 0.19003\tRecall: 0.13150\tF1: 0.15544\tF2: 0.14013\n",
      "\tTotal predictions: 15000\tTrue positives:  263\tFalse positives: 1121\tFalse negatives: 1737\tTrue negatives: 11879\n",
      "\n",
      "\n",
      "This took 114.21 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "3\n",
      "Number of component: 10\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.80187\tPrecision: 0.17162\tRecall: 0.12700\tF1: 0.14598\tF2: 0.13397\n",
      "\tTotal predictions: 15000\tTrue positives:  254\tFalse positives: 1226\tFalse negatives: 1746\tTrue negatives: 11774\n",
      "\n",
      "\n",
      "This took 113.89 seconds\n",
      "\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "approach2 = Pipeline([('reducer', PCA()),('clf', adb)])\n",
    "\n",
    "parameters = {'reducer__n_components':[1, 2, 3, 5, 7, 10]}\n",
    "  \n",
    "num = 1\n",
    "for features in [features_list, total_features, rel_total_features]:\n",
    "    print num\n",
    "    grid_search = GridSearchCV(approach2, parameters, scoring='f1')\n",
    "    start = time()\n",
    "    gird_result = grid_search.fit(df_scaled[features], df_scaled['poi']).best_estimator_\n",
    "    performance_w_pca(features, gird_result)\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Pipeline Results\n",
    "\n",
    "| Pipeline | Classifier | FeatureList | Accuracy | Precision | Recall | F1 | F2 | Seconds |\n",
    "|-----------|--------------|--------------------|----------|-----------|-----------|-----------|-----------|---------|\n",
    "| Approach1 | SVC | features_list | 0.774 | 0.307 | **0.551** | 0.394 | **0.475** | 108.9 |\n",
    "|  |  | total_features | 0.701 | 0.263 | **0.690** | 0.381 | 0.521 | 77.53 |\n",
    "|  |  | rel_total_features | 0.854 | **0.595** | **0.619** | **0.607** | **0.614** | 80.02 |\n",
    "|  | GaussianNB | features_list | 0.838 | 0.368 | 0.306 | 0.334 | 0.316 | 1.04 |\n",
    "|  |  | total_features | 0.840 | 0.377 | 0.311 | 0.341 | 0.322 | 1.28 |\n",
    "|  |  | rel_total_features | 0.469 | 0.148 | **0.625** | 0.239 | 0.379 | 1.09 |\n",
    "|  | Kneighbors | features_list | 0.861 | 0.348 | 0.046 | 0.081 | 0.056 | 13.92 |\n",
    "|  |  | total_features | 0.868 | **0.528** | 0.095 | 0.161 | 0.114 | 14.12 |\n",
    "|  |  | rel_total_features | 0.812 | 0.399 | 0.067 | 0.115 | 0.080 | 15.75 |\n",
    "|  | DecisionTree | features_list | 0.724 | 0.235 | 0.476 | 0.314 | 0.395 | 71.9 |\n",
    "|  |  | total_features | 0.778 | 0.318 | **0.579** | **0.410** | **0.497** | 72.44 |\n",
    "|  |  | rel_total_features | 0.838 | 0.335 | 0.220 | 0.266 | 0.236 | 73.66 |\n",
    "|  | RandomForest | features_list | 0.855 | 0.368 | 0.117 | 0.177 | 0.135 | 36.98 |\n",
    "|  |  | total_features | 0.862 | 0.458 | 0.172 | 0.250 | 0.196 | 37.75 |\n",
    "|  |  | rel_total_features | 0.814 | 0.305 | 0.093 | 0.142 | 0.107 | 37.32 |\n",
    "|  | AdaBoost | features_list | 0.850 | 0.415 | 0.299 | 0.348 | 0.317 | 118.86 |\n",
    "|  |  | total_features | 0.830 | 0.335 | 0.283 | 0.306 | 0.292 | 108.43 |\n",
    "|  |  | rel_total_features | 0.771 | 0.306 | 0.204 | 0.244 | 0.218 | 104.48 |\n",
    "| Approach2 | SVC | features_list | 0.736 | 0.312 | **0.813** | **0.451** | **0.615** | 31.94 |\n",
    "|  |  | total_features | 0.711 | 0.281 | **0.747** | **0.408** | **0.560** | 31.85 |\n",
    "|  |  | rel_total_features | 0.741 | 0.310 | **0.764** | **0.441** | **0.591** | 34.72 |\n",
    "|  | GaussianNB | features_list | 0.819 | 0.350 | 0.415 | 0.379 | 0.400 | 1.09 |\n",
    "|  |  | total_features | 0.831 | 0.383 | 0.435 | **0.407** | **0.423** | 1.11 |\n",
    "|  |  | rel_total_features | 0.754 | 0.182 | 0.242 | 0.208 | 0.227 | 1.15 |\n",
    "|  | Kneighbors | features_list | 0.868 | **0.513** | 0.195 | 0.282 | 0.222 | 20.62 |\n",
    "|  |  | total_features | 0.847 | 0.025 | 0.004 | 0.007 | 0.005 | 21.2 |\n",
    "|  |  | rel_total_features | 0.847 | 0.029 | 0.005 | 0.008 | 0.005 | 22.48 |\n",
    "|  | DecisionTree | features_list | 0.681 | 0.240 | **0.643** | 0.349 | **0.481** | 124.41 |\n",
    "|  |  | total_features | 0.800 | 0.233 | 0.217 | 0.225 | 0.220 | 125.89 |\n",
    "|  |  | rel_total_features | 0.824 | 0.258 | 0.172 | 0.206 | 0.184 | 117.21 |\n",
    "|  | RandomForest | features_list | 0.826 | 0.289 | 0.208 | 0.242 | 0.220 | 35.37 |\n",
    "|  |  | total_features | 0.823 | 0.298 | 0.241 | 0.267 | 0.251 | 34.91 |\n",
    "|  |  | rel_total_features | 0.841 | 0.114 | 0.028 | 0.045 | 0.033 | 35.96 |\n",
    "|  | AdaBoost | features_list | 0.822 | 0.266 | 0.190 | 0.222 | 0.202 | 104.41 |\n",
    "|  |  | total_features | 0.809 | 0.190 | 0.132 | 0.155 | 0.140 | 114.21 |\n",
    "|  |  | rel_total_features | 0.802 | 0.172 | 0.127 | 0.146 | 0.134 | 113.89 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features with F-value & p-value:\n",
      "1 ('rel_bonus', 20.988768488080161, 1.002166059752133e-05)\n",
      "2 ('fraction_to_poi', 16.641707070468989, 7.4941540250267645e-05)\n",
      "3 ('rel_long_term_incentive', 14.014032672700869, 0.00026283167217943732)\n",
      "4 ('shared_receipt_with_poi', 8.7464855321290802, 0.0036344020243633686)\n",
      "5 ('fraction_poi', 5.5185055438125357, 0.020194477662584531)\n",
      "6 ('rel_loan_advances', 5.396395592254871, 0.021598722340364536)\n",
      "7 ('from_poi_to_this_person', 5.3449415231473347, 0.022220727960811395)\n",
      "8 ('fraction_from_poi', 3.2107619169667667, 0.075284900599149329)\n",
      "9 ('rel_salary', 2.7730011744152487, 0.098071056290785164)\n",
      "10 ('from_this_person_to_poi', 2.4265081272428799, 0.12152433983710857)\n",
      "11 ('to_messages', 1.6988243485808538, 0.19455111487450777)\n",
      "12 ('rel_deferral_payments', 1.3381166890229022, 0.24930866438997767)\n",
      "13 ('rel_restricted_stock', 1.1488763692954786, 0.28560279187395243)\n",
      "14 ('rel_restricted_stock_deferred', 0.75851839176838753, 0.38526259639292249)\n",
      "15 ('rel_other', 0.71715043441877668, 0.3985051701615171)\n",
      "16 ('rel_director_fees', 0.20641384814063496, 0.65028654135700359)\n",
      "17 ('from_messages', 0.16416449823428589, 0.68596070789958996)\n",
      "18 ('rel_deferred_income', 0.1483995599012353, 0.70064593576041156)\n",
      "19 ('rel_exercised_stock_options', 0.027184689452379625, 0.86927464525919873)\n",
      "20 ('rel_expenses', 0.014418940564992357, 0.90459038090797916)\n"
     ]
    }
   ],
   "source": [
    "selected7 = selector.fit_transform(df_scaled[rel_total_features], df_scaled['poi'])\n",
    "scores = zip(rel_total_features, selector.scores_, selector.pvalues_)\n",
    "sorted_scores = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "print\"features with F-value & p-value:\"\n",
    "n=0\n",
    "while (n < len(sorted_scores)):\n",
    "    print n+1, sorted_scores[n]\n",
    "    n +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rel_bonus', 'fraction_to_poi', 'rel_long_term_incentive', 'shared_receipt_with_poi', 'fraction_poi', 'rel_loan_advances', 'from_poi_to_this_person']\n"
     ]
    }
   ],
   "source": [
    "finalized_features_list = list(map(lambda x: x[0], sorted_scores))[0:7]\n",
    "print(finalized_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rel_bonus: \"NaN\" disproportion biased feature\n",
    "- rel_loan_advances: the most \"NaN\" abundant feature\n",
    "\n",
    "## what about using email_features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features selected: 5\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.86311\tPrecision: 0.39219\tRecall: 0.42200\tF1: 0.40655\tF2: 0.41568\n",
      "\tTotal predictions: 9000\tTrue positives:  422\tFalse positives:  654\tFalse negatives:  578\tTrue negatives: 7346\n",
      "\n",
      "\n",
      "This took 67.80 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of features selected: 5\n",
      "SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.83522\tPrecision: 0.37134\tRecall: 0.69700\tF1: 0.48453\tF2: 0.59299\n",
      "\tTotal predictions: 9000\tTrue positives:  697\tFalse positives: 1180\tFalse negatives:  303\tTrue negatives: 6820\n",
      "\n",
      "\n",
      "This took 18.30 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of features selected: 5\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.84022\tPrecision: 0.20245\tRecall: 0.14900\tF1: 0.17166\tF2: 0.15731\n",
      "\tTotal predictions: 9000\tTrue positives:  149\tFalse positives:  587\tFalse negatives:  851\tTrue negatives: 7413\n",
      "\n",
      "\n",
      "This took 0.91 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of features selected: 5\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n",
      "\tAccuracy: 0.85789\tPrecision: 0.31123\tRecall: 0.23000\tF1: 0.26452\tF2: 0.24267\n",
      "\tTotal predictions: 9000\tTrue positives:  230\tFalse positives:  509\tFalse negatives:  770\tTrue negatives: 7491\n",
      "\n",
      "\n",
      "This took 12.93 seconds\n",
      "\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parameters = {svc: {'selector__k':[8, 7, 6, 5], \\\n",
    "                     'clf__kernel': ['rbf', 'linear', 'poly'], \\\n",
    "                     'clf__C': [0.1, 1, 10, 100, 1000], \\\n",
    "                     'clf__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \\\n",
    "                     'clf__class_weight': ['balanced', None]}, \\\n",
    "              gnb: {'selector__k':[8, 7, 6, 5]}, \\\n",
    "              neigh: {'selector__k':[8, 7, 6, 5], \\\n",
    "                      'clf__n_neighbors': [5, 8, 10, 15], \\\n",
    "                      'clf__weights' : ['uniform','distance'], \\\n",
    "                      'clf__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'], \\\n",
    "                      'clf__metric' : ['euclidean', 'manhattan', 'minkowski']}, \\\n",
    "              dt: {'selector__k':[8, 7, 6, 5], \\\n",
    "                  'clf__criterion': ['gini', 'entropy'], \\\n",
    "                  'clf__splitter': ['best', 'random'], \\\n",
    "                  'clf__max_features': [0.5, 'auto', 'log2', None], \\\n",
    "                  'clf__max_depth': [3, 5, 10, None], \\\n",
    "                  'clf__min_samples_leaf': [5, 4, 3, 2, 1], \\\n",
    "                  'clf__class_weight': ['balanced', None]}}\n",
    "\n",
    "\n",
    "for classifier in parameters:\n",
    "    approach1 = Pipeline([('selector', SelectKBest()), \\\n",
    "                      ('clf', classifier)])\n",
    "    grid_search = GridSearchCV(approach1, parameters[classifier], scoring='f1')\n",
    "    start = time()\n",
    "    gird_result = grid_search.fit(df_scaled[email_features], df_scaled['poi']).best_estimator_\n",
    "    performance(email_features, gird_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of component: 4\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.82940\tPrecision: 0.32254\tRecall: 0.25400\tF1: 0.28420\tF2: 0.26527\n",
      "\tTotal predictions: 15000\tTrue positives:  508\tFalse positives: 1067\tFalse negatives: 1492\tTrue negatives: 11933\n",
      "\n",
      "\n",
      "This took 102.93 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of component: 2\n",
      "SVC(C=10, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.76973\tPrecision: 0.32094\tRecall: 0.65150\tF1: 0.43003\tF2: 0.54022\n",
      "\tTotal predictions: 15000\tTrue positives: 1303\tFalse positives: 2757\tFalse negatives:  697\tTrue negatives: 10243\n",
      "\n",
      "\n",
      "This took 25.53 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of component: 5\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.78300\tPrecision: 0.10510\tRecall: 0.08350\tF1: 0.09306\tF2: 0.08708\n",
      "\tTotal predictions: 15000\tTrue positives:  167\tFalse positives: 1422\tFalse negatives: 1833\tTrue negatives: 11578\n",
      "\n",
      "\n",
      "This took 1.10 seconds\n",
      "\n",
      "--------------------------------------------------------\n",
      "Number of component: 5\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n",
      "\tAccuracy: 0.84500\tPrecision: 0.28923\tRecall: 0.11150\tF1: 0.16095\tF2: 0.12712\n",
      "\tTotal predictions: 15000\tTrue positives:  223\tFalse positives:  548\tFalse negatives: 1777\tTrue negatives: 12452\n",
      "\n",
      "\n",
      "This took 17.84 seconds\n",
      "\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "parameters = {svc: {'reducer__n_components':[1, 2, 3, 4, 5], \\\n",
    "                    'clf__kernel': ['rbf', 'linear', 'poly'], \\\n",
    "                    'clf__C': [0.1, 1, 10, 100, 1000], \\\n",
    "                    'clf__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \\\n",
    "                    'clf__class_weight': ['balanced', None]}, \\\n",
    "              gnb: {'reducer__n_components':[1, 2, 3, 4, 5]}, \\\n",
    "              neigh: {'reducer__n_components':[1, 2, 3, 4, 5], \\\n",
    "                      'clf__n_neighbors': [5, 8, 10, 15], \\\n",
    "                      'clf__weights' : ['uniform','distance'], \\\n",
    "                      'clf__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'], \\\n",
    "                      'clf__metric' : ['euclidean', 'manhattan', 'minkowski']}, \\\n",
    "              dt: {'reducer__n_components':[1, 2, 3, 4, 5], \\\n",
    "                   'clf__criterion': ['gini', 'entropy'], \\\n",
    "                   'clf__splitter': ['best', 'random'], \\\n",
    "                   'clf__max_features': [0.5, 'auto', 'log2', None], \\\n",
    "                   'clf__max_depth': [3, 5, 10, None], \\\n",
    "                   'clf__min_samples_leaf': [5, 4, 3, 2, 1], \\\n",
    "                   'clf__class_weight': ['balanced', None]}}\n",
    "\n",
    "\n",
    "for classifier in parameters:\n",
    "    approach2 = Pipeline([('reducer', PCA()), ('clf', classifier)])\n",
    "    grid_search = GridSearchCV(approach2, parameters[classifier], scoring='f1')\n",
    "    start = time()\n",
    "    gird_result = grid_search.fit(df_scaled[email_features], df_scaled['poi']).best_estimator_\n",
    "    performance_w_pca(email_features, gird_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Pipeline Results on email_features\n",
    "\n",
    "| Pipeline | Classifier | FeatureList | Accuracy | Precision | Recall | F1 | F2 | Seconds |\n",
    "|-----------|--------------|----------------|----------|-----------|--------|-------|-------|---------|\n",
    "| Approach1 | SVC | email_features | 0.835 | 0.371 | **0.697** | **0.485** | **0.593** | 18.3 |\n",
    "|  | GaussianNB | email_features | 0.840 | 0.202 | 0.149 | 0.172 | 0.157 | 0.91 |\n",
    "|  | Kneighbors | email_features | 0.858 | 0.311 | 0.230 | 0.265 | 0.243 | 12.93 |\n",
    "|  | DecisionTree | email_features | 0.863 | 0.392 | 0.422 | **0.407** | **0.416** | 67.8 |\n",
    "| Approach2 | SVC | email_features | 0.770 | 0.321 | **0.652** | **0.430** | **0.540** | 25.53 |\n",
    "|  | GaussianNB | email_features | 0.783 | 0.105 | 0.084 | 0.093 | 0.087 | 1.1 |\n",
    "|  | Kneighbors | email_features | 0.845 | 0.289 | 0.112 | 0.161 | 0.127 | 17.84 |\n",
    "|  | DecisionTree | email_features | 0.829 | 0.323 | 0.254 | 0.284 | 0.265 | 102.93 |\n",
    "\n",
    "The evaluating metric scores on email_feataures are relatively higher compared to total_features but do not exceed rel_total_features with SVC classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part5. Algorithm Selection\n",
    "\n",
    "## Best Feataure-Classifier Combination\n",
    "\n",
    "- Pipeline: Approach1\n",
    "- Classifier: SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "- FeatureLists:  ['rel_bonus', 'fraction_to_poi', 'rel_long_term_incentive', 'shared_receipt_with_poi', 'fraction_poi', 'rel_loan_advances', 'from_poi_to_this_person']\n",
    "- Result: \n",
    "\n",
    "| Accuracy: | Precision: | Recall: | F1: | F2: |\n",
    "|:---------:|:----------:|:-------:|:-----:|:-----:|\n",
    "| 0.854 | 0.595 | 0.619 | 0.607 | 0.614 |\n",
    "\n",
    "| Total predictions: | TRUE positives: | FALSE positives: | FALSE negatives: | TRUE negatives: |\n",
    "|:------------------:|:---------------:|:----------------:|:----------------:|:---------------:|\n",
    "| 11000 | 1237 | 842 | 763 | 8158 |\n",
    "\n",
    "### Finalize my_classifier.pkl, my_dataset.pkl, my_feature_list.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.85409\tPrecision: 0.59500\tRecall: 0.61850\tF1: 0.60652\tF2: 0.61365\n",
      "\tTotal predictions: 11000\tTrue positives: 1237\tFalse positives:  842\tFalse negatives:  763\tTrue negatives: 8158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_list = ['poi']+ finalized_features_list\n",
    "new_dataset = df_scaled[new_list].to_dict(orient = 'index')  \n",
    "new_clf = SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0, \\\n",
    "              decision_function_shape=None, degree=3, gamma=1, kernel='poly', \\\n",
    "              max_iter=-1, probability=False, random_state=None, \\\n",
    "              shrinking=True, tol=0.001, verbose=False)\n",
    "    \n",
    "tester.dump_classifier_and_data(new_clf, new_dataset, new_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Q5-1: did I have to do any scaling?\n",
    "Yes. I used MinMaxScaler to adjust financial (in $) and email (in count) features to be equally weighted and ranged between 0-1. Here, the scaled dataset is \"df_scaled.\"\n",
    "\n",
    "## Q5-2: what features did I end up using in your POI identifier?\n",
    "finalized_features_list = ['rel_bonus', 'fraction_to_poi', 'rel_long_term_incentive', 'shared_receipt_with_poi', 'fraction_poi', 'rel_loan_advances', 'from_poi_to_this_person']\n",
    "\n",
    "## Q5-3: what selection process did I use to pick features?\n",
    "I used SelectKBest() with f_classif score function to select 7 features among 20 features from rel_total_features. I have chosen f_classif because f-value can be used for continuous data, while chi-square is more appropriate for categorical data. The number 7 was resulted from grid search in the pipeline. \n",
    "\n",
    ">approach1 = Pipeline([('selector', SelectKBest()), ('clf', SVC())])\n",
    "\n",
    ">parameters = \\{'selector\\__k':[19, 15, 10, 7], \n",
    "                     'clf\\__kernel': ['rbf', 'linear', 'poly'], \n",
    "                     'clf\\__C': [0.1, 1, 10, 100, 1000], \n",
    "                     'clf\\__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "                     'clf\\__class_weight': ['balanced', None]\\}\n",
    "\n",
    ">grid_search = GridSearchCV(approach1, parameters, scoring='f1')\n",
    "\n",
    "I also tried PCA to reduce feature dimensions. By using PCA, the recall scores have improved from 0.6 to 0.8 for SVC classifiers, but the precision scores decreased from 0.5 to 0.3, where f1 and f2 scores remained similar or little bit lower compared to using SelectKBest method.\n",
    "\n",
    "## Q5-4: what algorithm did I end up using? \n",
    "\n",
    "Support vector machine classifier with degree 3 polynomial kernel.\n",
    "\n",
    "SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "\n",
    "\n",
    "## Q5-5: what other one(s) did I try?\n",
    "\n",
    "I have tried GaussianNB, KNeighbors, DecisionTrees, RandomForest, and Adaboost (boosted decision tree). \n",
    "\n",
    "\n",
    "## Q5-6: how did model performance differ between algorithms?\n",
    "\n",
    "I expected that KNeighbors and DecisionTrees perform poorly for this dataset because they tend to overfit for small dataset and increase bias for imbalanced classes. RandomForest and Adaboost were turned out to be the weakest algorithms for this task. The training speed and grid search speed were very slow for them, so I had to start with default parameters. To optimize RandomForest and Adaboost classifiers for this dataset seemed to require a large amount of time. I did not investigate on them further because their initial scores with default parameters were very low (f1 ranged between 0.1-0.3) and I think that they would not exceed what I got from SVC scores. \n",
    "\n",
    "## Q5-7: how did I tune the parameters of your particular algorithm?\n",
    "\n",
    "I used automated parameter search processes using GridSearchCV. See Q5-3.\n",
    "\n",
    "## Q5-8: what parameters did I tune?\n",
    "\n",
    "- C (penalty): 'clf__C': [0.1, 1, 10, **100**, 1000]\n",
    "- kernal: 'clf__kernel': ['rbf', 'linear', **'poly'**],\n",
    "- gamma (kernal coefficient): 'clf__gamma': [**1,** 0.1, 0.01, 0.001, 0.0001]\n",
    "- class_weight: 'clf__class_weight': [**'balanced'**, None]\n",
    "\n",
    "## Q5-9:  how did I validate the algorithm analysis?\n",
    "\n",
    "I used StratifiedShuffleSplit to validate the algorithms. Because the dataset has imbalanced classes, stratification based on class labels is required to ensure that relative class frequencies is approximately preserved in each train and test set.\n",
    "\n",
    "To evaluate the project, project reviewers use tester.py. Thus, it is convenient for me to use the similar validation method. The validation method used in tester.py is below.\n",
    "\n",
    ">cv = StratifiedShuffleSplit(labels, folds=1000, random_state = 42)\n",
    "\n",
    "I used random_state = 44 instead to avoid overfitting.\n",
    "\n",
    "## Q5-10: explain an interpretation of the metrics that says something human-understandable about the algorithms performance.\n",
    "\n",
    "The overall performance of the final SVC classifier to identify POI labels was ok but not excellent. The accuracy is based on both true POI and true non-POI labels and showed relatively high performance, where the scores were highly weighted by non-POI label with 87.5% of class size to total data points. If all people in the testing set (which was split by stratifying) are predicted to be non-POI, the accuracy will be as high as 87.5% regardless any feature values of individuals. Thus, the accuracy gave little information and it might be meaningless evaluation for the dataset with imbalanced classes. \n",
    "\n",
    "The precision score showed that 6 out of 10 predicted as POI were truly POI, while the rest were false positive. Low precision score will be costly in practice because we need to investigate a lot of non-POIs to catch small number of POI. This also will increase a chance of that innocent people get legal punishment. Among other classifiers, the final classifier resulted in the highest precision score, meaning that it would minimize chances of false positive cases and reduce cost to investigate non-POIs claimed to be POIs. \n",
    "\n",
    "The recall score showed that only 6 out of 10 true POIs were identified as POI, while the rest of true POIs were not identified as POI. If we rely on this classifier, we only can catch 62% of the bad guys and let 38% of the bad guys go. Some of SVC classifiers with PCA-tranformation had up to 0.8 of recall scores but their precision scores were as low as 0.3 as trade-off. In some cases, higher recall scores value more than higher precision, but for this project I would tend to think that the balanced scores between precision and recall can be more helpful.\n",
    "\n",
    "f1 (0.614) is about middle point of precision and recall scores, showing that overall performance of this classifier is OK, not too bad or not too great.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
